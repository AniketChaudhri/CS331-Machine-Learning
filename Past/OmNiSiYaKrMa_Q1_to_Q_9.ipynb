{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do install Follwoing libraries (if not already installed ) for smooth working of code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston , load_iris, load_digits\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import trange "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "\n",
    "1. Matrix Multiplication Layer\n",
    "2. Bias Addition Layer \n",
    "3. Mean Squared loss layer \n",
    "4. Softmax Activation \n",
    "5. Sigmoid Activation \n",
    "5. Cross Entropy Loss Layer \n",
    "6. Linear Activation \n",
    "7. tanh Activation \n",
    "8. ReLU Activation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a837fb",
   "metadata": {},
   "source": [
    "**1. Matrix Multiplication Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5416ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicationLayer : \n",
    "    \"\"\"\n",
    "    Inputs : X in R^(1xd) , W in R^(dxK)\n",
    "    This layer takes X & W as input and perform these 2 tasks: \n",
    "    1. Forward Pass : Matrix multiplication,  Z = XW \n",
    "    2. Backward Pass : dZ/dX , dZ/dW \n",
    "    \"\"\"\n",
    "    def __init__(self, X, W) : \n",
    "        self.X = X \n",
    "        self.W = W \n",
    "\n",
    "    def __str__(self,):\n",
    "        return \" An instance of Muliplication Layer.\"\n",
    "\n",
    "    def forward(self):  \n",
    "        self.Z = np.dot(self.X, self.W)\n",
    "\n",
    "    def backward(self):\n",
    "        self.dZ_dW = (self.X).T  # dZ/dW \n",
    "        self.dZ_daZ_prev = self.W  # dZ/dX "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546dc812",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f4e110",
   "metadata": {},
   "source": [
    "**2 Bias Addition Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c881f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdditionLayer : \n",
    "    \"\"\"\n",
    "    Inputs : Z in R^(1xK), B in R^(1xK)\n",
    "    This layer takes output Z of forward pass of Multiplication Layer as input and perform these 2 operations : \n",
    "    1. Forward Pass :  Z = Z + B\n",
    "    2. Backward Pass : dZ/dB\n",
    "    \"\"\"\n",
    "    def __init__(self, Z : np.ndarray , bias : np.ndarray ):\n",
    "        self.B = bias\n",
    "        self.Z = Z\n",
    "    \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Bias Addition Layer.\"\n",
    "    \n",
    "    def forward(self,):\n",
    "        self.Z = self.Z + self.B\n",
    "    \n",
    "    def backward(self,):\n",
    "        self.dZ_dB = np.identity( self.B.shape[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e58058",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec113e",
   "metadata": {},
   "source": [
    "**3. Mean Squared Loss Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f476834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredLossLayer : \n",
    "    \"\"\"\n",
    "    This layer implements Mean Square Loss Layer.\n",
    "    Inputs : Y in R^(1xK) , Y_hat in R^(1xK)  where K --> dimesion of output layer \n",
    "    This layer takes prediction Y_hat and true Y as input and perform these 2 opearations : \n",
    "    1. Forward Pass : L = (1/n) * || Y_hat - Y||**2 \n",
    "    2. Backward Pass : dL/dY_hat = (2/n)*(Y_hat - Y).T   Note :Here instead of dL/dY_hat , I used dL/daZ symbol which denote \n",
    "                                                             derivative of loss w.r.t. output of previous activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self, Y : np.ndarray , Y_hat : np.ndarray):\n",
    "        self.Y = Y \n",
    "        self.aZ = Y_hat \n",
    "    \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Mean Squared Loss Layer\"\n",
    "    \n",
    "    def forward(self, ):\n",
    "        self.L = np.mean( ( self.aZ - self.Y)**2 )\n",
    "        \n",
    "    def backward(self,):\n",
    "        self.dL_daZ = (2/len(self.Y))*(self.aZ - self.Y).T      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a381e7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca37b5d4",
   "metadata": {},
   "source": [
    "**4. Soft Max Activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed79a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxActivation : \n",
    "    \"\"\"\n",
    "    This layer implements SoftMax Activation Function.\n",
    "    Input : a numpy array Z in R^(1XK)  \n",
    "    1. Forward Pass : Apply Softmax Activation function, aZ = softmax(Z).T\n",
    "    2. Backward Pass : daZ/dZ  = diag(aZ) - sZ*transpose(aZ)  --> here diag(aZ) is diagonal matrix with \n",
    "                                                                   i-th diagnoal entry replaced by sZ_i value\n",
    "    \"\"\"\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z \n",
    "        \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Softmax Activation Layer\"\n",
    "        \n",
    "    def forward(self,):\n",
    "        self.aZ = self.softmax(self.Z)\n",
    "    \n",
    "    def backward(self,):\n",
    "        self.daZ_dZ = np.diag( self.aZ.reshape(-1) ) - (self.aZ.T)@( (self.aZ))  # Shape = (K,K) where K = len( sZ )\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(Z : np.ndarray):\n",
    "        max_Z = np.max( Z, axis=1 ,keepdims=True )\n",
    "        return (np.exp(Z - max_Z ))/np.sum( np.exp(Z - max_Z), axis=1 , keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b04aa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa13ef",
   "metadata": {},
   "source": [
    "**5. Sigmoid Activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60de0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidActivation :\n",
    "    \"\"\"\n",
    "    This layer implements Sigmoid Activation Function. \n",
    "    Input : a numpy array Z of shape Kx1 \n",
    "    1. Forward Pass : aZ = sigmoid( Z )  \n",
    "    2. Backward Pass : daZ/dZ = diagonal matrix with entries aZ_i*(1-aZ_i) --> sigZ_i means i-th component of sigZ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,Z ):\n",
    "        self.Z = Z \n",
    "    \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Sigmoid Activation Layer\"\n",
    "    \n",
    "    def forward(self,):\n",
    "        self.aZ = self.sigmoid( self.Z )  # sigmoid calculation\n",
    "    \n",
    "    def backward(self,):\n",
    "        diag_entries = np.multiply(self.aZ, 1-self.aZ).reshape(-1)\n",
    "        self.daZ_dZ = np.diag(diag_entries) \n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid( Z : np.ndarray ) :\n",
    "        return  1./(1 + np.exp(-Z) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa07a4c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb583e9",
   "metadata": {},
   "source": [
    "**6. Cross Entropy Loss Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd7235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossLayer : \n",
    "    \"\"\"\n",
    "    This layer implements Cross Entropy Loss Layer. \n",
    "    Inputs : Y in R^(1xK) , Y_pred in R^(1xK)  where K --> dimesion of output layer \n",
    "    This layer takes prediction Y_pred and true Y as input and perform these 2 opearations : \n",
    "    1. Forward Pass : L = -1 * dot product of Y & log(Y_pred)    \n",
    "    2. Backward Pass : dL/dY_pred in R^(Kx1)\n",
    "    \"\"\"    \n",
    "    def __init__(self, Y , Y_pred): \n",
    "        self.Y = Y\n",
    "        self.aZ = Y_pred\n",
    "        self.epsilon = 1e-40  \n",
    "        \n",
    "    \n",
    "    def __str__(self, ):\n",
    "        return \"An instance of Cross Entropy Loss Layer\"\n",
    "    \n",
    "    def forward(self, ):\n",
    "        self.L = - np.sum( self.Y * np.log(self.aZ+self.epsilon) )\n",
    "        \n",
    "    def backward(self, ):\n",
    "        self.dL_daZ = -1*(self.Y/(self.aZ + self.epsilon)).T # Element wise division"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824e020",
   "metadata": {},
   "source": [
    "**7. Linear Activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d14048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActivation : \n",
    "    \"\"\"\n",
    "    Implementation of linear activation function.\n",
    "    Input : Z in R^(1xn)\n",
    "    Ouput : linear(Z) = Z \n",
    "    \"\"\"\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z \n",
    "        \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Linear Activation.\"\n",
    "    \n",
    "    def forward(self, ):\n",
    "        self.aZ = self.Z \n",
    "    \n",
    "    def backward(self,):\n",
    "        self.daZ_dZ = np.identity( self.Z.shape[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc91c859",
   "metadata": {},
   "source": [
    "**8. tanh Activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8da04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tanhActivation : \n",
    "    \"\"\"\n",
    "    Implementation of tanh activation function\n",
    "    Input : a numpy array Z in R^(1xK)\n",
    "    1. Forward Pass : aZ = tanh(Z)\n",
    "    2. Backward Pass : daZ/dZ = np.diag(1 - aZ**2)   --> R^(KxK)\n",
    "    \"\"\"\n",
    "    def __init__(self, Z): \n",
    "        self.Z = Z \n",
    "        \n",
    "    def __str__(self,): \n",
    "        return \"An instance of tanhActivation class.\"\n",
    "    \n",
    "    def forward(self,): \n",
    "        self.aZ = np.tanh(self.Z)\n",
    "    \n",
    "    def backward(self,): \n",
    "        self.daZ_dZ = np.diag(1 - self.aZ.reshape(-1)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc8820",
   "metadata": {},
   "source": [
    "**9. ReLUActivation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7f4d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUActivation : \n",
    "    \"\"\"\n",
    "    Implementation of relu activatino function\n",
    "    Input : a numpy array Z in R^(1xK)\n",
    "    1. Forward Pass aZ = max(Z,0)\n",
    "    2. Backward Pass : daZ_dZ = diag_matrix( 1 if aZ_i>0 else 0 )\n",
    "    \"\"\"\n",
    "    def __init__(self, Z): \n",
    "        self.Z = Z \n",
    "        self.Leak = 0.01\n",
    "    \n",
    "    def __str__(self,):\n",
    "        return \"An instance of ReLU activation\"\n",
    "    \n",
    "    def forward(self,):\n",
    "        self.aZ = np.maximum(self.Z,0)\n",
    "    \n",
    "    def backward(self,):\n",
    "        self.daZ_dZ = np.diag( [1. if x>=0 else self.Leak for x in self.aZ.reshape(-1)])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 & 3 \n",
    "\n",
    "- Question 2 :  Boston House Price Prediction \n",
    "- Question 3 -- MNIST Hand Written Digit Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data and Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name='boston', \n",
    "             normalize_X=False, \n",
    "             normalize_y=False,\n",
    "             one_hot_encode_y = False, \n",
    "             test_size=0.2):\n",
    "    if dataset_name == 'boston' : \n",
    "        data = load_boston()\n",
    "    elif dataset_name == 'iris' : \n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'mnist':\n",
    "        data = load_digits()\n",
    "        data['data'] = 1*(data['data']>=8)\n",
    "\n",
    "    X = data['data']\n",
    "    y = data['target'].reshape(-1,1)\n",
    "    \n",
    "    if normalize_X == True : \n",
    "        normalizer = Normalizer()\n",
    "        X  = normalizer.fit_transform(X)\n",
    "    \n",
    "    if normalize_y == True : \n",
    "        normalizer = Normalizer()\n",
    "        y = normalizer.fit_transform(y)\n",
    "    \n",
    "    if one_hot_encode_y == True : \n",
    "        encoder = OneHotEncoder()\n",
    "        y = encoder.fit_transform(y).toarray()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent ( SGD )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_pass(X_sample, Y_sample, W, B, activation='linear', loss='mean_squared'):\n",
    "    multiply_layer = MultiplicationLayer(X_sample, W)\n",
    "    multiply_layer.forward()\n",
    "\n",
    "    bias_add_layer = BiasAdditionLayer(multiply_layer.Z, B)\n",
    "    bias_add_layer.forward()\n",
    "\n",
    "    if activation == 'linear' : \n",
    "        activation_layer = LinearActivation(bias_add_layer.Z)\n",
    "    elif activation == 'softmax': \n",
    "        activation_layer = SoftMaxActivation(bias_add_layer.Z)\n",
    "    activation_layer.forward()\n",
    "    \n",
    "    if loss == 'mean_squared' :\n",
    "        loss_layer = MeanSquaredLossLayer(Y_sample, activation_layer.aZ )\n",
    "    elif loss=='cross_entropy' : \n",
    "        loss_layer = CrossEntropyLossLayer(Y_sample, activation_layer.aZ )\n",
    "    loss_layer.forward()\n",
    "    \n",
    "    \n",
    "    return multiply_layer, bias_add_layer, activation_layer, loss_layer\n",
    "\n",
    "def backward_pass(multiply_layer, bias_add_layer, activation_layer, loss_layer): \n",
    "\n",
    "    loss_layer.backward()\n",
    "    activation_layer.backward()\n",
    "    bias_add_layer.backward()\n",
    "    multiply_layer.backward()\n",
    "\n",
    "    return loss_layer, activation_layer, bias_add_layer, multiply_layer \n",
    "\n",
    "\n",
    "def StochasticGradientDescent( X_train,\n",
    "                               y_train, \n",
    "                               X_test, \n",
    "                               y_test, \n",
    "                               inp_shape = 1,   # dimension of input \n",
    "                               out_shape = 1,   # dimension of output  \n",
    "                               n_iterations = 10000,\n",
    "                               learning_rate = 0.01,\n",
    "                               activation = 'linear',\n",
    "                               loss = 'mean_squared',\n",
    "                               seed = 42,\n",
    "                               task='regression'  #  one of  [ 'regression', 'classification' ]\n",
    "                            ):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # initialize W & B \n",
    "    W_shape = ( inp_shape,  out_shape )\n",
    "    B_shape = ( 1, out_shape )\n",
    "\n",
    "    W = np.random.random(W_shape)\n",
    "    B  = np.random.random(B_shape)\n",
    "\n",
    "    iterations = trange(n_iterations ,desc=\"Training...\", ncols=100)\n",
    "\n",
    "    for iteration, _ in enumerate(iterations) : \n",
    "        randomIndx = np.random.randint( len(X_train) )\n",
    "        X_sample = X_train[randomIndx, :].reshape(1, inp_shape)\n",
    "        Y_sample = y_train[randomIndx, :].reshape(1, out_shape)\n",
    "\n",
    "        # Forward Pass\n",
    "        # 1) Z <-- XW \n",
    "        # 2) Z <-- Z + Bias\n",
    "        # 3) Z <-- activation( Z ) \n",
    "        # 4) find Loss L \n",
    "\n",
    "        multiply_layer, bias_add_layer, activation_layer, loss_layer = forward_pass(X_sample, Y_sample, W, B, activation,loss)\n",
    "\n",
    "        # Note : here whenever I write aZ it means it is output of some activation function applied on Z \n",
    "\n",
    "        # Backward Pass \n",
    "        # 1) dL/daZ \n",
    "        # 2) dL/dZ = dL/daZ* daZ/dZ \n",
    "        # 3) dL/dW = dZ/dW * dL/dZ \n",
    "        # 4) dL/dB = dZ/dB * dL/dB \n",
    "        \n",
    "        loss_layer, activation_layer, bias_add_layer, multiply_layer = backward_pass(multiply_layer, bias_add_layer, activation_layer, loss_layer)\n",
    "        \n",
    "        dL_daZ = loss_layer.dL_daZ \n",
    "        dL_dZ = np.dot( activation_layer.daZ_dZ, dL_daZ ) \n",
    "        dL_dW = np.dot( multiply_layer.dZ_dW , dL_dZ.T)\n",
    "        dL_dB = np.dot( bias_add_layer.dZ_dB, dL_dZ).T\n",
    "\n",
    "        # Update W & B \n",
    "        W -=  learning_rate*dL_dW \n",
    "        B -=  learning_rate*dL_dB\n",
    "        \n",
    "        if iteration%1000 == 0 : \n",
    "            iterations.set_description( \"Sample Error : %0.5f\"%loss_layer.L, refresh=True )\n",
    "    \n",
    "    # Lets run forward pass for train and test data and check accuracy/error\n",
    "\n",
    "\n",
    "    if task =='regression':\n",
    "        if isinstance(loss_layer, MeanSquaredLossLayer) : \n",
    "            _ , _, _,  loss_layer = forward_pass( X_train, y_train , W, B, activation, loss)\n",
    "            print(\"Mean Squared Loss Error (Train Data)  : %0.5f\"% loss_layer.L)\n",
    "                        \n",
    "            _ , _, _,  loss_layer = forward_pass( X_test, y_test , W, B, activation, loss)\n",
    "            print(\"Mean Squared Loss error (Test Data) : %0.5f\"%loss_layer.L)\n",
    "    \n",
    "    if task =='classification': \n",
    "        if isinstance(loss_layer, CrossEntropyLossLayer): \n",
    "            y_true = np.argmax(y_train, axis=1)\n",
    "            _, _, _, loss_layer = forward_pass( X_train, y_train , W, B, activation, loss)\n",
    "            y_pred = np.argmax( loss_layer.aZ, axis=1)\n",
    "\n",
    "            acc = 1*(y_pred == y_true)\n",
    "            print(\"Classification Accuracy (Training Data ): {0}/{1} = {2} %\".format(sum(acc), len(acc), sum(acc)*100/len(acc)))\n",
    "\n",
    "            y_true = np.argmax(y_test,axis=1)\n",
    "            _, _, _, loss_layer = forward_pass( X_test, y_test , W, B, activation, loss)\n",
    "            y_pred = np.argmax( loss_layer.aZ, axis=1)\n",
    "\n",
    "            acc = 1*(y_pred == y_true)\n",
    "            print(\"Classification Accuracy (Testing Data ): {0}/{1} = {2} %\".format(sum(acc), len(acc), sum(acc)*100/len(acc)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7e1a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test  = load_data('boston', normalize_X=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample Error : 3.51963: 100%|██████████████████████████████| 10000/10000 [00:00<00:00, 27320.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 61.26899\n",
      "Mean Squared Loss error (Test Data) : 58.32278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "StochasticGradientDescent(X_train, y_train, X_test, y_test, inp_shape=X_train.shape[1], out_shape=y_train.shape[1], task='regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data('iris',normalize_X=True, one_hot_encode_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample Error : 0.89642: 100%|████████████████████████████████| 5000/5000 [00:00<00:00, 19016.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Training Data ): 79/120 = 65.83333333333333 %\n",
      "Classification Accuracy (Testing Data ): 21/30 = 70.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "StochasticGradientDescent(X_train,y_train,X_test,y_test, inp_shape=X_train.shape[1], \\\n",
    "                          out_shape=y_train.shape[1], \n",
    "                          n_iterations=5000,\n",
    "                          learning_rate=0.001,\n",
    "                          activation='softmax',\n",
    "                          task='classification',\n",
    "                          loss='cross_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 & 5 & 6\n",
    "\n",
    "1. Question 4 - Implement neural network \n",
    "2. Question 5 - predicting boston house price using different neural netowork architectures \n",
    "3. Question 6 - classifying digits in MNIST dataset using different neural network architectures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer :\n",
    "    \"\"\"\n",
    "    Input - activation : Activation Layer Name ,n_inp : dimension of input ,  n_out :  Number of output neurons \n",
    "    \"\"\" \n",
    "    def __init__(self, n_inp, n_out, activation_name = \"linear\" , seed=42 ):\n",
    "        \n",
    "        np.random.seed(seed) # for reproducability of code\n",
    "\n",
    "        self.n_inp = n_inp\n",
    "        self.n_out = n_out \n",
    "        \n",
    "        # random initialization of input X  and output Z \n",
    "        self.X = np.random.random( (1, n_inp))   # assigned during SGD \n",
    "        self.Z  = np.random.random( (1, n_out ))  \n",
    "\n",
    "        # Initialize W & B with some scaling to avoid over-flow \n",
    "        self.W  =  np.random.random( (n_inp, n_out) )* np.sqrt( 2 / ( n_inp + n_out) )\n",
    "        self.B  = np.random.random( (1,n_out) )*np.sqrt( 2 / (1 + n_out ) )\n",
    "\n",
    "        # define multiplication layer, bias addition layer , and activation layer \n",
    "        self.multiply_layer = MultiplicationLayer(self.X, self.W) \n",
    "        self.bias_add_layer = BiasAdditionLayer( self.B, self.B )  \n",
    "\n",
    "        if activation_name == 'linear' : \n",
    "            self.activation_layer = LinearActivation(self.Z)\n",
    "        elif activation_name == 'sigmoid' : \n",
    "            self.activation_layer = SigmoidActivation(self.Z)\n",
    "        elif activation_name == 'softmax' : \n",
    "            self.activation_layer = SoftMaxActivation(self.Z)\n",
    "        elif activation_name == 'tanh' : \n",
    "            self.activation_layer = tanhActivation(self.Z)\n",
    "        elif activation_name == 'relu' : \n",
    "            self.activation_layer = ReLUActivation(self.Z)\n",
    "\n",
    "    def forward(self,): \n",
    "        self.multiply_layer.X = self.X\n",
    "        self.multiply_layer.forward()\n",
    "\n",
    "        self.bias_add_layer.Z = self.multiply_layer.Z \n",
    "        self.bias_add_layer.forward() \n",
    "\n",
    "        self.activation_layer.Z = self.bias_add_layer.Z \n",
    "        self.activation_layer.forward() \n",
    "\n",
    "        self.Z = self.activation_layer.aZ  # output of given layer \n",
    "\n",
    "\n",
    "    def backward(self,):\n",
    "        self.activation_layer.backward() \n",
    "        self.bias_add_layer.backward()\n",
    "        self.multiply_layer.backward()      \n",
    "    \n",
    "\n",
    "class NeuralNetwork(Layer) : \n",
    "    \"\"\"\n",
    "    Input  - layers : list of layer objects , loss_name : Name of loss layer\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, loss_name = \"mean_squared\", learning_rate = 0.01, seed=42):   # [ \"mean_squared\", \"cross_entropy\"]\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)  # number of layers in neural network \n",
    "        self.learning_rate = learning_rate \n",
    "\n",
    "        self.inp_shape = self.layers[0].X.shape \n",
    "        self.out_shape = self.layers[-1].Z.shape\n",
    "\n",
    "        # random initialization of input X  and output Z \n",
    "        self.X = np.random.random( self.inp_shape)   # assigned during SGD \n",
    "        self.Y  = np.random.random( self.out_shape )  # output of neural network \n",
    "\n",
    "        #define loss layer \n",
    "        if loss_name == \"mean_squared\" : \n",
    "            self.loss_layer = MeanSquaredLossLayer( self.Y , self.Y )\n",
    "        if loss_name == \"cross_entropy\" : \n",
    "            self.loss_layer = CrossEntropyLossLayer( self.Y, self.Y )\n",
    "            \n",
    "        \n",
    "    def forward(self,): \n",
    "        self.layers[0].X = self.X\n",
    "        self.loss_layer.Y = self.Y \n",
    "         \n",
    "        self.layers[0].forward()\n",
    "        for i in range(1, self.n_layers ): \n",
    "            self.layers[i].X = self.layers[i-1].Z  \n",
    "            self.layers[i].forward()\n",
    "            \n",
    "        self.loss_layer.aZ = self.layers[-1].Z \n",
    "        self.loss_layer.forward() \n",
    "    \n",
    "    def backward(self,): \n",
    "        \n",
    "        self.loss_layer.Z = self.Y \n",
    "        self.loss_layer.backward() \n",
    "        self.grad_nn = self.loss_layer.dL_daZ\n",
    "        for i in range( self.n_layers-1, -1, -1 ): \n",
    "            self.layers[i].backward()\n",
    "             \n",
    "            dL_dZ =  np.dot( self.layers[i].activation_layer.daZ_dZ, self.grad_nn ) \n",
    "            dL_dW =  np.dot( self.layers[i].multiply_layer.dZ_dW , dL_dZ.T)\n",
    "            dL_dB =  np.dot( self.layers[i].bias_add_layer.dZ_dB, dL_dZ).T\n",
    "            \n",
    "            # Update W & B \n",
    "            self.layers[i].W -=  self.learning_rate*dL_dW \n",
    "            self.layers[i].B -=  self.learning_rate*dL_dB\n",
    "\n",
    "            # Update outer_grad \n",
    "            self.grad_nn  = np.dot( self.layers[i].multiply_layer.dZ_daZ_prev, dL_dZ )\n",
    "\n",
    "            del dL_dZ, dL_dW, dL_dB        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLayers(inp_shape, layers_sizes, layers_activations): \n",
    "    layers = []\n",
    "    n_layers = len(layers_sizes)\n",
    "    layer_0 = Layer( inp_shape, layers_sizes[0], layers_activations[0] ) \n",
    "    layers.append(layer_0)\n",
    "    inp_shape_next = layers_sizes[0]\n",
    "    for  i in range(1,n_layers): \n",
    "        layer_i = Layer( inp_shape_next, layers_sizes[i], layers_activations[i] )\n",
    "        layers.append(layer_i)\n",
    "        inp_shape_next = layers_sizes[i]\n",
    "    \n",
    "    out_shape = inp_shape_next  \n",
    "    return inp_shape, out_shape, layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_NeuralNetwork( X_train,\n",
    "                       y_train,\n",
    "                       X_test,\n",
    "                       y_test,\n",
    "                       nn,\n",
    "                       inp_shape = 1,   # dimension of input \n",
    "                       out_shape = 1,   # dimension of output \n",
    "                       n_iterations = 1000, \n",
    "                       task = \"regression\"  # [ \"regression\", \"classification\"]\n",
    "                       ):\n",
    "    iterations = trange(n_iterations,desc=\"Training ...\", ncols=100)\n",
    "\n",
    "    for iteration, _ in enumerate(iterations): \n",
    "        randomIndx = np.random.randint( len(X_train) )\n",
    "        X_sample = X_train[randomIndx, :].reshape(1, inp_shape)\n",
    "        Y_sample = y_train[randomIndx, :].reshape(1, out_shape)\n",
    "\n",
    "        nn.X = X_sample  \n",
    "        nn.Y = Y_sample \n",
    "\n",
    "        nn.forward()  # Forward Pass \n",
    "        nn.backward() # Backward Pass \n",
    "    \n",
    "    # Lets run ONLY forward pass for train and test data and check accuracy/error \n",
    "\n",
    "    if task == \"regression\": \n",
    "        nn.X = X_train \n",
    "        nn.Y = y_train \n",
    "        nn.forward() \n",
    "        train_error = nn.loss_layer.L \n",
    "        nn.X = X_test \n",
    "        nn.Y = y_test\n",
    "         \n",
    "        nn.forward()\n",
    "        \n",
    "        test_error = nn.loss_layer.L \n",
    "\n",
    "        if isinstance(nn.loss_layer , MeanSquaredLossLayer): \n",
    "            print(\"Mean Squared Loss Error (Train Data)  : %0.5f\"% train_error)\n",
    "            print(\"Mean Squared Loss Error (Test Data)  : %0.5f\"% test_error)\n",
    "\n",
    "    if task == \"classification\": \n",
    "        nn.X = X_train \n",
    "        nn.Y = y_train \n",
    "        nn.forward()\n",
    "        y_true = np.argmax( y_train, axis=1) \n",
    "        y_pred = np.argmax( nn.loss_layer.aZ, axis=1)\n",
    "        acc = 1*(y_true == y_pred)\n",
    "        print(\"Classification Accuracy (Training Data ): {0}/{1} = {2} %\".format(sum(acc), len(acc), sum(acc)*100/len(acc)))\n",
    "\n",
    "        nn.X = X_test \n",
    "        nn.Y = y_test \n",
    "        nn.forward()\n",
    "        y_true = np.argmax( y_test, axis=1) \n",
    "        y_pred = np.argmax( nn.loss_layer.aZ, axis=1)\n",
    "        acc = 1*(y_true == y_pred)\n",
    "        print(\"Classification Accuracy (Testing Data ): {0}/{1} = {2} %\".format(sum(acc), len(acc), sum(acc)*100/len(acc)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test  = load_data('boston', normalize_X=True, normalize_y=False, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heads Up : If you think that mean square error are high  just normalize_y = True in above line and see Magic !!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Netowrk Architecture 1 \n",
    "\n",
    "- 1 Layer \n",
    "- Layer 1 with one output neuron and linear activation \n",
    "- Mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|████████████████████████████████████████| 11111/11111 [00:00<00:00, 26723.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 56.95890\n",
      "Mean Squared Loss Error (Test Data)  : 54.84047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [1]\n",
    "layers_activations = ['linear']\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mean_squared'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.1)\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=11111,task=\"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Architecture 2 \n",
    "- 2 Layers \n",
    "- Layer 1 with 13 output neurons and sigmoid activation\n",
    "- Layer 2 with 1 output with linear activation\n",
    "- Mean Squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████████████████████████████████████| 1000/1000 [00:00<00:00, 14778.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 64.88076\n",
      "Mean Squared Loss Error (Test Data)  : 63.65933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [13,1]\n",
    "layers_activations = ['sigmoid','linear']\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mean_squared'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.01)\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=1000,task=\"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Architecture 3 \n",
    "- 3 Layers \n",
    "- Layer 1 with 13 output neurons and sigmoid activation\n",
    "- Layer 2 with 13 output neurons and sigmoid activation\n",
    "- Layer 3 with 1 output neuron and linear activation \n",
    "- Mean Squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████████████████████████████████████| 1000/1000 [00:00<00:00, 10241.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 83.19427\n",
      "Mean Squared Loss Error (Test Data)  : 90.22964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [13,13,1]\n",
    "layers_activations = ['sigmoid','sigmoid','linear']\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mean_squared'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.001)\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=1000,task=\"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data('mnist', one_hot_encode_y=True, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Netowrk Architecture 1 \n",
    "\n",
    "- 2 Layer \n",
    "- Layer 1 with 89 output neurons and tanh activation \n",
    "- Layer 2 with 10 output neurons and sigmoid activation \n",
    "- Mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|████████████████████████████████████████| 10000/10000 [00:00<00:00, 11620.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Training Data ): 1194/1257 = 94.98806682577566 %\n",
      "Classification Accuracy (Testing Data ): 490/540 = 90.74074074074075 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [89,10]\n",
    "layers_activations = ['tanh','sigmoid']\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mean_squared'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.1)\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=10000,task=\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Netowrk Architecture 2 \n",
    "\n",
    "- 2 Layer \n",
    "- Layer 1 with 89 output neurons and tanh activation \n",
    "- Layer 2 with 10 output neurons and (linear activation + softmax activation) = softmax activation \n",
    "- Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|████████████████████████████████████████| 11111/11111 [00:01<00:00, 10346.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Training Data ): 1201/1257 = 95.54494828957836 %\n",
      "Classification Accuracy (Testing Data ): 494/540 = 91.48148148148148 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [89,10]\n",
    "layers_activations = ['tanh','softmax']\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'cross_entropy'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.01)\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=11111,task=\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Convolutional Layer for 1-channel input and 1 channel output + flatten operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__All the assumptions are taken for 1 channel input and (n = 1) channel output__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assuming that we have filter, input- inp\n",
    "define filter for convolutional layer, we are fixing the size to be 3x3 and stride to be 1.\n",
    "if it is not then we may face difficulty in finding proper zero-padding for the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next question we have implemented multi-channel input and multi-channel out with filter-size given by user. Stride in next question is also \n",
    "taken 1 as not only we will face finding proper zero-padding for input but also gradient calculation will become more tricky in backpropagation. \n",
    "For sake of simplicity we only considered stride=1 case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming we are given single channel input and initial filter to be a 3x3 matrix:\n",
    "\n",
    "def convolutional_layer(zero_pad_input, l_filter):\n",
    "    \n",
    "    l = len(inp) #length of input matrix\n",
    "    m = len(l_filter)  #length of filter\n",
    "    c = len(zero_pad_input)  #size of zero-padded matrix\n",
    "    s  = (c - m) + 1  #to be used for loop for filtering\n",
    "    out = np.zeros((l, l))  #output after convolution\n",
    "\n",
    "    #filtering-\n",
    "    for i in range(s):\n",
    "        for j in range(s):\n",
    "            temp = np.zeros((m,m))\n",
    "            row, col = np.indices((m,m))\n",
    "            temp = np.multiply(zero_pad_input[row+i, col+j], l_filter)\n",
    "            \n",
    "            out[i][j] = np.sum(temp)\n",
    "            \n",
    "    return out\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#Forward pass implementation-\n",
    "def Forward_pass(inp, l_filter):\n",
    "    l = len(inp)\n",
    "    #Zero-padding of input layer-\n",
    "    zero_pad_input = np.zeros((l+2, l+2))\n",
    "    zero_pad_input[ 1:l+1, 1:l+1] = inp\n",
    "    \n",
    "    f_out = convolutional_layer(zero_pad_input, l_filter)    \n",
    "    return f_out\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "# Function to Rotate\n",
    "# the matrix by 180 degree\n",
    "def rotateMatrix(mat):\n",
    "    N = len(mat)\n",
    "    rot_mat = np.zeros((N,N))\n",
    "    k = N - 1\n",
    "    t1 = 0\n",
    "    while(k >= 0 and t1 < 3):\n",
    "        j = N - 1;\n",
    "        t2 = 0\n",
    "        while(j >= 0 and t2 < N):\n",
    "            rot_mat[t1][t2] = mat[k][j]\n",
    "            j = j - 1\n",
    "            t2 = t2 + 1\n",
    "        k = k - 1\n",
    "        t1 = t1 + 1\n",
    "     \n",
    "    return rot_mat\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Backward pass implementation-\n",
    "\n",
    "def Backward_pass(inp, output, l_filter):\n",
    "    l = len(inp)\n",
    "\n",
    "#--------------------------------Backward Pass---------------------------------------\n",
    "    #Zero-padding of input layer-\n",
    "    zero_pad_input = np.zeros((l+2, l+2))\n",
    "    zero_pad_input[ 1:l+1, 1:l+1] = inp\n",
    "    \n",
    "    grad_filter = convolutional_layer(zero_pad_input, output)\n",
    "    #we can use gradient of filter coefficient matrix to update the filter matrix: \n",
    "    #-- l_filter - l_filter - alpha*grad_filter ,where alpha is learning rate\n",
    "    \n",
    "    #for gradient of loss w.r.t input, we need to rotate the filter by 180° and apply convolution.\n",
    "    rotated_filter = rotateMatrix(l_filter)\n",
    "    zero_pad_output = np.zeros((l+2, l+2))\n",
    "    zero_pad_output[ 1:l+1, 1:l+1] = output\n",
    "    grad_X = convolutional_layer(zero_pad_output, rotated_filter)\n",
    "    \n",
    "    return grad_filter, grad_X\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#flatten operation:\n",
    "\n",
    "def flatten(inp_mat):\n",
    "    flatten_vector = []\n",
    "    \n",
    "    for i in range(len(inp_mat)):  #number of rows\n",
    "        for j in range(len(inp_mat[0])):  #number of columns\n",
    "            flatten_vector.append(inp_mat[i][j])\n",
    "    \n",
    "    flatten_vector = np.array(flatten_vector)\n",
    "    return flatten_vector\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output for forward pass [[  4.   6.   8.   4.]\n",
      " [ 10. 101.   7.   8.]\n",
      " [  9.  13. 104.   5.]\n",
      " [  1.   9.  11. 101.]]\n"
     ]
    }
   ],
   "source": [
    "inp = np.array([[1,2,3,4],[2,3,4,5],[7,8,97,1],[1,2,3,4]])\n",
    "l_filter = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "forward_out = Forward_pass(inp,l_filter)\n",
    "print('output for forward pass', forward_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss w.r.t filter from Backward pass: \n",
      " [[10445.  2010.  1842.]\n",
      " [ 2031. 11163.  2037.]\n",
      " [ 1827.  2010. 10433.]]\n",
      "Gradient of loss w.r.t input from Backward pass: \n",
      " [[105.  13.  16.   4.]\n",
      " [ 23. 209.  18.  16.]\n",
      " [ 18.  34. 306.  12.]\n",
      " [  1.  18.  24. 205.]]\n"
     ]
    }
   ],
   "source": [
    "dL_df, dL_dX= Backward_pass(inp,forward_out,l_filter)\n",
    "t = np.zeros((3,3))\n",
    "t = dL_df[:3, :3]  #assuming filter is of size 3x3\n",
    "dL_df = t\n",
    "\n",
    "print('Gradient of loss w.r.t filter from Backward pass:', '\\n', dL_df)\n",
    "print('Gradient of loss w.r.t input from Backward pass:', '\\n',dL_dX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 & 9\n",
    "\n",
    "- Question 8 : Convolutional Neural Network with multi-channel input and multi-channel output. \n",
    "               filter shape can be given by user (default = (1,1))\n",
    "               Stride is taken as 1 for reason explained in previous question\n",
    "- Question 9 : MNIST hand written digits classification using CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 : Convolutional Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer : \n",
    "    \"\"\"\n",
    "    Implementation of Convolutional Layer consist of Convolution  followed by flattening  and Activation operation\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "            inp_shape ,                                              # inp_shape = (input_channels, input_height, input_width )\n",
    "            activation ='tanh' , \n",
    "            filter_shape = (1,1),                                    # filter_shape = (filter_height, filter_width)\n",
    "            lr = 0.01,\n",
    "            Co = 1 ,\n",
    "            seed = 42):                                                # number of output channels \n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        # Check if filter is valid or NOT \n",
    "        assert ( inp_shape[1]>=filter_shape[0] and inp_shape[2]>= filter_shape[1]) , \\\n",
    "             \"Error : Input {} incompatible with filter {}\".format(inp.shape, filter_shape)\n",
    "\n",
    "        self.inp = np.random.rand(*inp_shape) \n",
    "        self.inp_shape = inp_shape\n",
    "        self.Ci = self.inp.shape[0]                                    # number of channels in input here denoted as inp \n",
    "        self.Co = Co                                                   # number of output channels \n",
    "        self.filters_shape = ( self.Co , self.Ci,  *filter_shape )        \n",
    "        self.out_shape = (self.Co, self.inp.shape[1] - filter_shape[0] + 1, self.inp.shape[2] - filter_shape[1] + 1)\n",
    "        self.flatten_shape = self.out_shape[0]*self.out_shape[1]*self.out_shape[2]\n",
    "        self.lr = lr \n",
    "\n",
    "        # Randomly initialize filters, biases, output, flatten output \n",
    "        self.filters = np.random.rand( *self.filters_shape )  \n",
    "        self.biases  = np.random.rand( *self.out_shape )\n",
    "        self.out = np.random.rand( *self.out_shape )\n",
    "        self.flatten_out = np.random.rand(1,self.flatten_shape) \n",
    "\n",
    "        # Define activation function \n",
    "        if activation == 'tanh': \n",
    "            self.activation_layer = tanhActivation( self.out )\n",
    "    \n",
    "\n",
    "    def forward(self, ) :        \n",
    "        self.out = np.copy( self.biases )  # add bias to output \n",
    "        for i in range( self.Co ) : \n",
    "            for j in range( self.Ci ) : \n",
    "                self.out[i] += self.convolve(self.inp[j], self.filters[i,j])\n",
    "                \n",
    "        self.flatten()\n",
    "        self.activation_layer.Z = self.flatten_out \n",
    "        self.activation_layer.forward() \n",
    "\n",
    "    def backward(self, grad_nn ): \n",
    "        \n",
    "        self.activation_layer.backward()\n",
    "        loss_gradient = np.dot( self.activation_layer.daZ_dZ, grad_nn )\n",
    "        loss_gradient = np.reshape(loss_gradient, self.out_shape)  # reshape to (Co, H_out, W_out)\n",
    "        \n",
    "        self.filters_gradient = np.zeros( self.filters_shape )  # dL/dKij for each filter  Kij    1<=i<=Ci , 1<=j<=Co \n",
    "        self.input_gradient = np.zeros( self.inp_shape )  #  dL/dXj \n",
    "        self.biases_gradient = loss_gradient  # dL/dBi  = dL/dYi \n",
    "        padded_loss_gradient = np.pad( loss_gradient, ((0,0), (self.filters_shape[2]-1,self.filters_shape[2]-1 ), (self.filters_shape[3]-1,self.filters_shape[3]-1 )))\n",
    "        \n",
    "        for i in range(self.Co):\n",
    "            for j in range( self.Ci ): \n",
    "                self.filters_gradient[i,j] = self.convolve( self.inp[j], loss_gradient[i] )  # dL/dKij = convolution( Xj, dL/dYi)\n",
    "                rot180_Kij = np.rot90( np.rot90( self.filters[i,j], axes=(0,1) ) , axes=(0,1) )  \n",
    "                self.input_gradient[j] += self.convolve( padded_loss_gradient[i], rot180_Kij )   # dL/dXj = convolution ( padded dL/dYi , Kij rotated by 180 anit-clockwise )\n",
    "        \n",
    "        # update filters and biases \n",
    "        self.filters -= self.lr*self.filters_gradient \n",
    "        self.biases -= self.lr*self.biases_gradient\n",
    "\n",
    "\n",
    "    # flattening output to 1 Dimension so it can be fed int neural network \n",
    "    def flatten(self, ): \n",
    "        self.flatten_out = self.out.reshape(1,-1)\n",
    "    \n",
    "    # convolutional operation with stride=1  \n",
    "    def convolve(self, x, y ):\n",
    "        x_conv_y  = np.zeros((x.shape[0] - y.shape[0] +1 , x.shape[1] - y.shape[1] + 1))\n",
    "        for i in range(x.shape[0]-y.shape[0] + 1)  :\n",
    "            for j in range( x.shape[1] - y.shape[1] + 1) : \n",
    "                tmp = x[i:i+y.shape[0], j:j+y.shape[1] ]\n",
    "                tmp = np.multiply(tmp, y)\n",
    "                x_conv_y[i,j] = np.sum( tmp )\n",
    "        return x_conv_y   \n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN : \n",
    "    \"\"\"\n",
    "    Implementation of Convolutional Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                convolutional_layer,                   # convolutional layer \n",
    "                nn,                                    # feed forward neural network\n",
    "                seed = 42): \n",
    "\n",
    "        self.nn = nn \n",
    "        self.convolutional_layer = convolutional_layer \n",
    "        self.X = _ # assigned during SGD \n",
    "        self.Y = _ # assigned during SGD \n",
    "    \n",
    "    def forward(self,):\n",
    "        # forward pass of convolutional layer \n",
    "        self.convolutional_layer.inp = self.X \n",
    "        self.convolutional_layer.forward()\n",
    "\n",
    "        # forward pass of neural network \n",
    "        self.nn.X = self.convolutional_layer.activation_layer.aZ\n",
    "        self.nn.Y = self.Y \n",
    "        self.nn.forward()  \n",
    "    \n",
    "    def backward(self,): \n",
    "        # backward pass of neural network \n",
    "        self.nn.backward() \n",
    "\n",
    "        # backward pass of convolutional network \n",
    "        self.convolutional_layer.backward( self.nn.grad_nn )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_CNN(X_train,\n",
    "            y_train, \n",
    "            X_test, \n",
    "            y_test, \n",
    "            cnn, \n",
    "            inp_shape, \n",
    "            out_shape,\n",
    "            n_iterations=1000,\n",
    "            task=\"classification\"):\n",
    "    \n",
    "    iterations = trange(n_iterations,desc=\"Training ...\", ncols=100)\n",
    "\n",
    "    for iteration, _ in enumerate(iterations): \n",
    "        randomIndx = np.random.randint( len(X_train) )\n",
    "        X_sample = X_train[randomIndx, :].reshape(inp_shape)\n",
    "        Y_sample = y_train[randomIndx, :].reshape(out_shape)\n",
    "\n",
    "        cnn.X = X_sample  \n",
    "        cnn.Y = Y_sample \n",
    "\n",
    "        cnn.forward()  # Forward Pass \n",
    "        cnn.backward() # Backward Pass \n",
    "    \n",
    "    # Lets run ONLY forward pass for train and test data and check accuracy/error \n",
    "\n",
    "    if task == \"classification\": \n",
    "        X_train = X_train.reshape(-1,8,8)\n",
    "        y_true = np.argmax( y_train, axis=1) \n",
    "        acc = 0 \n",
    "        for i in range( len(X_train) ): \n",
    "            cnn.X = X_train[i][np.newaxis, :, :]\n",
    "            cnn.Y = y_train[i]\n",
    "            cnn.forward()\n",
    "            y_pred_i = np.argmax( cnn.nn.loss_layer.aZ, axis=1)\n",
    "            if (y_pred_i == y_true[i]) : acc += 1\n",
    "        print(\"Classification Accuracy (Training Data ): {0}/{1} = {2} %\".format(acc, len(y_true), acc*100/len(y_true)))\n",
    "\n",
    "        X_test = X_test.reshape(-1,8,8)\n",
    "        y_true = np.argmax( y_test, axis=1) \n",
    "        acc = 0 \n",
    "        for i in range( len(X_test) ): \n",
    "            cnn.X = X_test[i][np.newaxis, :, :]\n",
    "            cnn.Y = y_test[i]\n",
    "            cnn.forward()\n",
    "            y_pred_i = np.argmax( cnn.nn.loss_layer.aZ, axis=1)\n",
    "            if (y_pred_i == y_true[i]) : acc += 1\n",
    "        print(\"Classification Accuracy (Testing Data ): {0}/{1} = {2} %\".format(acc, len(y_true), acc*100/len(y_true)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 : MNIST hand written digit classification using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random((2,3,4)).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data('mnist', one_hot_encode_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Architecture \n",
    "\n",
    "- 2 Layers \n",
    "- Layer 1 with 16 output channels + flatten + tanh activation \n",
    "- Layer 2 with 10 output neuron with  (linear activation + softmax activation ) = softmax activation \n",
    "- Cross Entropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|█████████████████████████████████████████████| 5000/5000 [01:05<00:00, 76.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Training Data ): 1295/1437 = 90.11830201809325 %\n",
      "Classification Accuracy (Testing Data ): 333/360 = 92.5 %\n"
     ]
    }
   ],
   "source": [
    "conv_inp_shape = (1,8,8)   # sklearn digit dataset has images of shape 1 x 8 x 8\n",
    "Co = 16  # 16 channel output \n",
    "conv_filter_shape = (3,3)\n",
    "conv_activation = 'tanh'\n",
    "convolutional_layer = ConvolutionalLayer(conv_inp_shape, \n",
    "                                        filter_shape = conv_filter_shape, \n",
    "                                        Co = Co,\n",
    "                                        activation = conv_activation,\n",
    "                                        lr = 0.01)\n",
    "nn_inp_shape = convolutional_layer.flatten_shape \n",
    "layers_sizes = [10]\n",
    "layers_activations = ['softmax']\n",
    "\n",
    "nn_inp_shape, nn_out_shape, layers = createLayers(nn_inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'cross_entropy'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.01)\n",
    "\n",
    "cnn = CNN( convolutional_layer, nn)\n",
    "out_shape =  (1, layers_sizes[-1])  # one_hot encoded ouptut \n",
    "\n",
    "SGD_CNN(X_train,y_train,X_test,y_test, cnn,conv_inp_shape, out_shape,n_iterations=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbaff3bad43e72ef86a08724e2a04c1dc7b916dad0faa2935f126c230f07c1b0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
