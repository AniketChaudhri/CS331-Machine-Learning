{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do install Follwoing libraries (if not already installed ) for smooth working of code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\python310\\lib\\site-packages (0.0.post1)\n",
      "Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (1.4.3)\n",
      "Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (1.22.4)\n",
      "Requirement already satisfied: tqdm in c:\\python310\\lib\\site-packages (4.64.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from tqdm) (0.4.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install sklearn pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston , load_iris, load_digits\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import trange "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "\n",
    "1. Matrix Multiplication Layer\n",
    "2. Bias Addition Layer \n",
    "3. Mean Squared loss layer \n",
    "4. Softmax Activation \n",
    "5. Sigmoid Activation \n",
    "5. Cross Entropy Loss Layer \n",
    "6. Linear Activation \n",
    "7. tanh Activation \n",
    "8. ReLU Activation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a837fb",
   "metadata": {},
   "source": [
    "**1. Matrix Multiplication Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5416ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicationLayer : \n",
    "    \"\"\"\n",
    "    Inputs : X in R^(1xd) , W in R^(dxK)\n",
    "    This layer takes X & W as input and perform these 2 tasks: \n",
    "    1. Forward Pass : Matrix multiplication,  Z = XW \n",
    "    2. Backward Pass : dZ/dX , dZ/dW \n",
    "    \"\"\"\n",
    "    def __init__(self, X, W) : \n",
    "        self.X = X \n",
    "        self.W = W \n",
    "\n",
    "    def __str__(self,):\n",
    "        return \" An instance of Muliplication Layer.\"\n",
    "\n",
    "    def forward(self):  \n",
    "        self.Z = np.dot(self.X, self.W)\n",
    "\n",
    "    def backward(self):\n",
    "        self.dZ_dW = (self.X).T  # dZ/dW \n",
    "        self.dZ_daZ_prev = self.W  # dZ/dX "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546dc812",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f4e110",
   "metadata": {},
   "source": [
    "**2 Bias Addition Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c881f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdditionLayer : \n",
    "    \"\"\"\n",
    "    Inputs : Z in R^(1xK), B in R^(1xK)\n",
    "    This layer takes output Z of forward pass of Multiplication Layer as input and perform these 2 operations : \n",
    "    1. Forward Pass :  Z = Z + B\n",
    "    2. Backward Pass : dZ/dB\n",
    "    \"\"\"\n",
    "    def __init__(self, Z : np.ndarray , bias : np.ndarray ):\n",
    "        self.B = bias\n",
    "        self.Z = Z\n",
    "    \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Bias Addition Layer.\"\n",
    "    \n",
    "    def forward(self,):\n",
    "        self.Z = self.Z + self.B\n",
    "    \n",
    "    def backward(self,):\n",
    "        self.dZ_dB = np.identity( self.B.shape[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e58058",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec113e",
   "metadata": {},
   "source": [
    "**3. Mean Squared Loss Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f476834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredLossLayer : \n",
    "    \"\"\"\n",
    "    This layer implements Mean Square Loss Layer.\n",
    "    Inputs : Y in R^(1xK) , Y_hat in R^(1xK)  where K --> dimesion of output layer \n",
    "    This layer takes prediction Y_hat and true Y as input and perform these 2 opearations : \n",
    "    1. Forward Pass : L = (1/n) * || Y_hat - Y||**2 \n",
    "    2. Backward Pass : dL/dY_hat = (2/n)*(Y_hat - Y).T   Note :Here instead of dL/dY_hat , I used dL/daZ symbol which denote \n",
    "                                                             derivative of loss w.r.t. output of previous activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self, Y : np.ndarray , Y_hat : np.ndarray):\n",
    "        self.Y = Y \n",
    "        self.aZ = Y_hat \n",
    "    \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Mean Squared Loss Layer\"\n",
    "    \n",
    "    def forward(self, ):\n",
    "        self.L = np.mean( ( self.aZ - self.Y)**2 )\n",
    "        \n",
    "    def backward(self,):\n",
    "        self.dL_daZ = (2/len(self.Y))*(self.aZ - self.Y).T      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a381e7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca37b5d4",
   "metadata": {},
   "source": [
    "**4. Soft Max Activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed79a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxActivation : \n",
    "    \"\"\"\n",
    "    This layer implements SoftMax Activation Function.\n",
    "    Input : a numpy array Z in R^(1XK)  \n",
    "    1. Forward Pass : Apply Softmax Activation function, aZ = softmax(Z).T\n",
    "    2. Backward Pass : daZ/dZ  = diag(aZ) - sZ*transpose(aZ)  --> here diag(aZ) is diagonal matrix with \n",
    "                                                                   i-th diagnoal entry replaced by sZ_i value\n",
    "    \"\"\"\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z \n",
    "        \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Softmax Activation Layer\"\n",
    "        \n",
    "    def forward(self,):\n",
    "        self.aZ = self.softmax(self.Z)\n",
    "    \n",
    "    def backward(self,):\n",
    "        self.daZ_dZ = np.diag( self.aZ.reshape(-1) ) - (self.aZ.T)@( (self.aZ))  # Shape = (K,K) where K = len( sZ )\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(Z : np.ndarray):\n",
    "        max_Z = np.max( Z, axis=1 ,keepdims=True )\n",
    "        return (np.exp(Z - max_Z ))/np.sum( np.exp(Z - max_Z), axis=1 , keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b04aa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa13ef",
   "metadata": {},
   "source": [
    "**5. Sigmoid Activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60de0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidActivation :\n",
    "    \"\"\"\n",
    "    This layer implements Sigmoid Activation Function. \n",
    "    Input : a numpy array Z of shape Kx1 \n",
    "    1. Forward Pass : aZ = sigmoid( Z )  \n",
    "    2. Backward Pass : daZ/dZ = diagonal matrix with entries aZ_i*(1-aZ_i) --> sigZ_i means i-th component of sigZ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,Z ):\n",
    "        self.Z = Z \n",
    "    \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Sigmoid Activation Layer\"\n",
    "    \n",
    "    def forward(self,):\n",
    "        self.aZ = self.sigmoid( self.Z )  # sigmoid calculation\n",
    "    \n",
    "    def backward(self,):\n",
    "        diag_entries = np.multiply(self.aZ, 1-self.aZ).reshape(-1)\n",
    "        self.daZ_dZ = np.diag(diag_entries) \n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid( Z : np.ndarray ) :\n",
    "        return  1./(1 + np.exp(-Z) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa07a4c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb583e9",
   "metadata": {},
   "source": [
    "**6. Cross Entropy Loss Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd7235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossLayer : \n",
    "    \"\"\"\n",
    "    This layer implements Cross Entropy Loss Layer. \n",
    "    Inputs : Y in R^(1xK) , Y_pred in R^(1xK)  where K --> dimesion of output layer \n",
    "    This layer takes prediction Y_pred and true Y as input and perform these 2 opearations : \n",
    "    1. Forward Pass : L = -1 * dot product of Y & log(Y_pred)    \n",
    "    2. Backward Pass : dL/dY_pred in R^(Kx1)\n",
    "    \"\"\"    \n",
    "    def __init__(self, Y , Y_pred): \n",
    "        self.Y = Y\n",
    "        self.aZ = Y_pred\n",
    "        self.epsilon = 1e-40  \n",
    "        \n",
    "    \n",
    "    def __str__(self, ):\n",
    "        return \"An instance of Cross Entropy Loss Layer\"\n",
    "    \n",
    "    def forward(self, ):\n",
    "        self.L = - np.sum( self.Y * np.log(self.aZ+self.epsilon) )\n",
    "        \n",
    "    def backward(self, ):\n",
    "        self.dL_daZ = -1*(self.Y/(self.aZ + self.epsilon)).T # Element wise division"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824e020",
   "metadata": {},
   "source": [
    "**7. Linear Activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d14048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActivation : \n",
    "    \"\"\"\n",
    "    Implementation of linear activation function.\n",
    "    Input : Z in R^(1xn)\n",
    "    Ouput : linear(Z) = Z \n",
    "    \"\"\"\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z \n",
    "        \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Linear Activation.\"\n",
    "    \n",
    "    def forward(self, ):\n",
    "        self.aZ = self.Z \n",
    "    \n",
    "    def backward(self,):\n",
    "        self.daZ_dZ = np.identity( self.Z.shape[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc91c859",
   "metadata": {},
   "source": [
    "**8. tanh Activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8da04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tanhActivation : \n",
    "    \"\"\"\n",
    "    Implementation of tanh activation function\n",
    "    Input : a numpy array Z in R^(1xK)\n",
    "    1. Forward Pass : aZ = tanh(Z)\n",
    "    2. Backward Pass : daZ/dZ = np.diag(1 - aZ**2)   --> R^(KxK)\n",
    "    \"\"\"\n",
    "    def __init__(self, Z): \n",
    "        self.Z = Z \n",
    "        \n",
    "    def __str__(self,): \n",
    "        return \"An instance of tanhActivation class.\"\n",
    "    \n",
    "    def forward(self,): \n",
    "        self.aZ = np.tanh(self.Z)\n",
    "    \n",
    "    def backward(self,): \n",
    "        self.daZ_dZ = np.diag(1 - self.aZ.reshape(-1)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc8820",
   "metadata": {},
   "source": [
    "**9. ReLUActivation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7f4d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUActivation : \n",
    "    \"\"\"\n",
    "    Implementation of relu activatino function\n",
    "    Input : a numpy array Z in R^(1xK)\n",
    "    1. Forward Pass aZ = max(Z,0)\n",
    "    2. Backward Pass : daZ_dZ = diag_matrix( 1 if aZ_i>0 else 0 )\n",
    "    \"\"\"\n",
    "    def __init__(self, Z): \n",
    "        self.Z = Z \n",
    "        self.Leak = 0.01\n",
    "    \n",
    "    def __str__(self,):\n",
    "        return \"An instance of ReLU activation\"\n",
    "    \n",
    "    def forward(self,):\n",
    "        self.aZ = np.maximum(self.Z,0)\n",
    "    \n",
    "    def backward(self,):\n",
    "        self.daZ_dZ = np.diag( [1. if x>=0 else self.Leak for x in self.aZ.reshape(-1)])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 & 3 \n",
    "\n",
    "- Question 2 :  Boston House Price Prediction \n",
    "- Question 3 -- MNIST Hand Written Digit Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data and Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name='boston', \n",
    "             normalize_X=False, \n",
    "             normalize_y=False,\n",
    "             one_hot_encode_y = False, \n",
    "             test_size=0.2):\n",
    "    if dataset_name == 'boston' : \n",
    "        data = load_boston()\n",
    "    elif dataset_name == 'iris' : \n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'mnist':\n",
    "        data = load_digits()\n",
    "        data['data'] = 1*(data['data']>=8)\n",
    "\n",
    "    X = data['data']\n",
    "    y = data['target'].reshape(-1,1)\n",
    "    \n",
    "    if normalize_X == True : \n",
    "        normalizer = Normalizer()\n",
    "        X  = normalizer.fit_transform(X)\n",
    "    \n",
    "    if normalize_y == True : \n",
    "        normalizer = Normalizer()\n",
    "        y = normalizer.fit_transform(y)\n",
    "    \n",
    "    if one_hot_encode_y == True : \n",
    "        encoder = OneHotEncoder()\n",
    "        y = encoder.fit_transform(y).toarray()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent ( SGD )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_pass(X_sample, Y_sample, W, B, activation='linear', loss='mean_squared'):\n",
    "    multiply_layer = MultiplicationLayer(X_sample, W)\n",
    "    multiply_layer.forward()\n",
    "\n",
    "    bias_add_layer = BiasAdditionLayer(multiply_layer.Z, B)\n",
    "    bias_add_layer.forward()\n",
    "\n",
    "    if activation == 'linear' : \n",
    "        activation_layer = LinearActivation(bias_add_layer.Z)\n",
    "    elif activation == 'softmax': \n",
    "        activation_layer = SoftMaxActivation(bias_add_layer.Z)\n",
    "    activation_layer.forward()\n",
    "    \n",
    "    if loss == 'mean_squared' :\n",
    "        loss_layer = MeanSquaredLossLayer(Y_sample, activation_layer.aZ )\n",
    "    elif loss=='cross_entropy' : \n",
    "        loss_layer = CrossEntropyLossLayer(Y_sample, activation_layer.aZ )\n",
    "    loss_layer.forward()\n",
    "    \n",
    "    \n",
    "    return multiply_layer, bias_add_layer, activation_layer, loss_layer\n",
    "\n",
    "def backward_pass(multiply_layer, bias_add_layer, activation_layer, loss_layer): \n",
    "\n",
    "    loss_layer.backward()\n",
    "    activation_layer.backward()\n",
    "    bias_add_layer.backward()\n",
    "    multiply_layer.backward()\n",
    "\n",
    "    return loss_layer, activation_layer, bias_add_layer, multiply_layer \n",
    "\n",
    "\n",
    "def StochasticGradientDescent( X_train,\n",
    "                               y_train, \n",
    "                               X_test, \n",
    "                               y_test, \n",
    "                               inp_shape = 1,   # dimension of input \n",
    "                               out_shape = 1,   # dimension of output  \n",
    "                               n_iterations = 10000,\n",
    "                               learning_rate = 0.01,\n",
    "                               activation = 'linear',\n",
    "                               loss = 'mean_squared',\n",
    "                               seed = 42,\n",
    "                               task='regression'  #  one of  [ 'regression', 'classification' ]\n",
    "                            ):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # initialize W & B \n",
    "    W_shape = ( inp_shape,  out_shape )\n",
    "    B_shape = ( 1, out_shape )\n",
    "\n",
    "    W = np.random.random(W_shape)\n",
    "    B  = np.random.random(B_shape)\n",
    "\n",
    "    iterations = trange(n_iterations ,desc=\"Training...\", ncols=100)\n",
    "\n",
    "    for iteration, _ in enumerate(iterations) : \n",
    "        randomIndx = np.random.randint( len(X_train) )\n",
    "        X_sample = X_train[randomIndx, :].reshape(1, inp_shape)\n",
    "        Y_sample = y_train[randomIndx, :].reshape(1, out_shape)\n",
    "\n",
    "        # Forward Pass\n",
    "        # 1) Z <-- XW \n",
    "        # 2) Z <-- Z + Bias\n",
    "        # 3) Z <-- activation( Z ) \n",
    "        # 4) find Loss L \n",
    "\n",
    "        multiply_layer, bias_add_layer, activation_layer, loss_layer = forward_pass(X_sample, Y_sample, W, B, activation,loss)\n",
    "\n",
    "        # Note : here whenever I write aZ it means it is output of some activation function applied on Z \n",
    "\n",
    "        # Backward Pass \n",
    "        # 1) dL/daZ \n",
    "        # 2) dL/dZ = dL/daZ* daZ/dZ \n",
    "        # 3) dL/dW = dZ/dW * dL/dZ \n",
    "        # 4) dL/dB = dZ/dB * dL/dB \n",
    "        \n",
    "        loss_layer, activation_layer, bias_add_layer, multiply_layer = backward_pass(multiply_layer, bias_add_layer, activation_layer, loss_layer)\n",
    "        \n",
    "        dL_daZ = loss_layer.dL_daZ \n",
    "        dL_dZ = np.dot( activation_layer.daZ_dZ, dL_daZ ) \n",
    "        dL_dW = np.dot( multiply_layer.dZ_dW , dL_dZ.T)\n",
    "        dL_dB = np.dot( bias_add_layer.dZ_dB, dL_dZ).T\n",
    "\n",
    "        # Update W & B \n",
    "        W -=  learning_rate*dL_dW \n",
    "        B -=  learning_rate*dL_dB\n",
    "        \n",
    "        if iteration%1000 == 0 : \n",
    "            iterations.set_description( \"Sample Error : %0.5f\"%loss_layer.L, refresh=True )\n",
    "    \n",
    "    # Lets run forward pass for train and test data and check accuracy/error\n",
    "\n",
    "\n",
    "    if task =='regression':\n",
    "        if isinstance(loss_layer, MeanSquaredLossLayer) : \n",
    "            _ , _, _,  loss_layer = forward_pass( X_train, y_train , W, B, activation, loss)\n",
    "            print(\"Mean Squared Loss Error (Train Data)  : %0.5f\"% loss_layer.L)\n",
    "                        \n",
    "            _ , _, _,  loss_layer = forward_pass( X_test, y_test , W, B, activation, loss)\n",
    "            print(\"Mean Squared Loss error (Test Data) : %0.5f\"%loss_layer.L)\n",
    "    \n",
    "    if task =='classification': \n",
    "        if isinstance(loss_layer, CrossEntropyLossLayer): \n",
    "            y_true = np.argmax(y_train, axis=1)\n",
    "            _, _, _, loss_layer = forward_pass( X_train, y_train , W, B, activation, loss)\n",
    "            y_pred = np.argmax( loss_layer.aZ, axis=1)\n",
    "\n",
    "            acc = 1*(y_pred == y_true)\n",
    "            print(\"Classification Accuracy (Training Data ): {0}/{1} = {2} %\".format(sum(acc), len(acc), sum(acc)*100/len(acc)))\n",
    "\n",
    "            y_true = np.argmax(y_test,axis=1)\n",
    "            _, _, _, loss_layer = forward_pass( X_test, y_test , W, B, activation, loss)\n",
    "            y_pred = np.argmax( loss_layer.aZ, axis=1)\n",
    "\n",
    "            acc = 1*(y_pred == y_true)\n",
    "            print(\"Classification Accuracy (Testing Data ): {0}/{1} = {2} %\".format(sum(acc), len(acc), sum(acc)*100/len(acc)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7e1a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test  = load_data('boston', normalize_X=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample Error : 59.10494: 100%|█████████████████████████████| 10000/10000 [00:00<00:00, 21034.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 62.92856\n",
      "Mean Squared Loss error (Test Data) : 49.46550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "StochasticGradientDescent(X_train, y_train, X_test, y_test, inp_shape=X_train.shape[1], out_shape=y_train.shape[1], task='regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data('iris',normalize_X=True, one_hot_encode_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample Error : 0.89642: 100%|████████████████████████████████| 5000/5000 [00:00<00:00, 13051.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Training Data ): 79/120 = 65.83333333333333 %\n",
      "Classification Accuracy (Testing Data ): 21/30 = 70.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "StochasticGradientDescent(X_train,y_train,X_test,y_test, inp_shape=X_train.shape[1], \\\n",
    "                          out_shape=y_train.shape[1], \n",
    "                          n_iterations=5000,\n",
    "                          learning_rate=0.001,\n",
    "                          activation='softmax',\n",
    "                          task='classification',\n",
    "                          loss='cross_entropy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
