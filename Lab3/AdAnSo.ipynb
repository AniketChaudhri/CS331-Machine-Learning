{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team: AdAnSo\n",
    "\n",
    "Members:\n",
    "1. Adarsh Anand (2003101)\n",
    "2. Aniket Chaudhri (2003104)\n",
    "3. Somesh Agrawal (2003326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder\n",
    "from tqdm import trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicationLayer:\n",
    "    \"\"\"\n",
    "    Inputs : X in R^(1xd) , W in R^(dxK)\n",
    "    This layer takes X & W as input and perform these 2 tasks: \n",
    "    1. Forward Pass : Matrix multiplication,  Z = XW \n",
    "    2. Backward Pass : dZ/dX , dZ/dW \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, W):\n",
    "        self.X = X\n",
    "        self.W = W\n",
    "\n",
    "    def __str__(self,):\n",
    "        return \" An instance of Muliplication Layer.\"\n",
    "\n",
    "    def forward(self):\n",
    "        self.Z = np.dot(self.X, self.W)\n",
    "\n",
    "    def backward(self):\n",
    "        self.dZ_dW = (self.X).T  # dZ/dW\n",
    "        self.dZ_daZ_prev = self.W  # dZ/dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdditionLayer : \n",
    "    \"\"\"\n",
    "    Inputs : Z in R^(1xK), B in R^(1xK)\n",
    "    This layer takes output Z of forward pass of Multiplication Layer as input and perform these 2 operations : \n",
    "    1. Forward Pass :  Z = Z + B\n",
    "    2. Backward Pass : dZ/dB\n",
    "    \"\"\"\n",
    "    def __init__(self, Z : np.ndarray , bias : np.ndarray ):\n",
    "        self.B = bias\n",
    "        self.Z = Z\n",
    "    \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Bias Addition Layer.\"\n",
    "    \n",
    "    def forward(self,):\n",
    "        self.Z = self.Z + self.B\n",
    "    \n",
    "    def backward(self,):\n",
    "        self.dZ_dB = np.identity( self.B.shape[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredLossLayer:\n",
    "    \"\"\"\n",
    "    This layer implements Mean Square Loss Layer.\n",
    "    Inputs : Y in R^(1xK) , Y_hat in R^(1xK)  where K --> dimesion of output layer \n",
    "    This layer takes prediction Y_hat and true Y as input and perform these 2 opearations : \n",
    "    1. Forward Pass : L = (1/n) * || Y_hat - Y||**2 \n",
    "    2. Backward Pass : dL/dY_hat = (2/n)*(Y_hat - Y).T   Note :Here instead of dL/dY_hat , I used dL/daZ symbol which denote \n",
    "                                                             derivative of loss w.r.t. output of previous activation layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Y: np.ndarray, Y_hat: np.ndarray):\n",
    "        self.Y = Y\n",
    "        self.aZ = Y_hat\n",
    "\n",
    "    def __str__(self,):\n",
    "        return \"An instance of Mean Squared Loss Layer\"\n",
    "\n",
    "    def forward(self, ):\n",
    "        self.L = np.mean((self.aZ - self.Y)**2)\n",
    "\n",
    "    def backward(self,):\n",
    "        self.dL_daZ = (2/len(self.Y))*(self.aZ - self.Y).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxActivation : \n",
    "    \"\"\"\n",
    "    This layer implements SoftMax Activation Function.\n",
    "    Input : a numpy array Z in R^(1XK)  \n",
    "    1. Forward Pass : Apply Softmax Activation function, aZ = softmax(Z).T\n",
    "    2. Backward Pass : daZ/dZ  = diag(aZ) - sZ*transpose(aZ)  --> here diag(aZ) is diagonal matrix with \n",
    "                                                                   i-th diagnoal entry replaced by sZ_i value\n",
    "    \"\"\"\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z \n",
    "        \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Softmax Activation Layer\"\n",
    "        \n",
    "    def forward(self,):\n",
    "        self.aZ = self.softmax(self.Z)\n",
    "    \n",
    "    def backward(self,):\n",
    "        self.daZ_dZ = np.diag( self.aZ.reshape(-1) ) - (self.aZ.T)@( (self.aZ))  # Shape = (K,K) where K = len( sZ )\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(Z : np.ndarray):\n",
    "        max_Z = np.max( Z, axis=1 ,keepdims=True )\n",
    "        return (np.exp(Z - max_Z ))/np.sum( np.exp(Z - max_Z), axis=1 , keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidActivation:\n",
    "    \"\"\"\n",
    "    This layer implements Sigmoid Activation Function. \n",
    "    Input : a numpy array Z of shape Kx1 \n",
    "    1. Forward Pass : aZ = sigmoid( Z )  \n",
    "    2. Backward Pass : daZ/dZ = diagonal matrix with entries aZ_i*(1-aZ_i) --> sigZ_i means i-th component of sigZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z\n",
    "\n",
    "    def __str__(self,):\n",
    "        return \"An instance of Sigmoid Activation Layer\"\n",
    "\n",
    "    def forward(self,):\n",
    "        self.aZ = self.sigmoid(self.Z)  # sigmoid calculation\n",
    "\n",
    "    def backward(self,):\n",
    "        diag_entries = np.multiply(self.aZ, 1-self.aZ).reshape(-1)\n",
    "        self.daZ_dZ = np.diag(diag_entries)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(Z: np.ndarray):\n",
    "        return 1./(1 + np.exp(-Z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossLayer:\n",
    "    \"\"\"\n",
    "    This layer implements Cross Entropy Loss Layer. \n",
    "    Inputs : Y in R^(1xK) , Y_pred in R^(1xK)  where K --> dimesion of output layer \n",
    "    This layer takes prediction Y_pred and true Y as input and perform these 2 opearations : \n",
    "    1. Forward Pass : L = -1 * dot product of Y & log(Y_pred)    \n",
    "    2. Backward Pass : dL/dY_pred in R^(Kx1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Y, Y_pred):\n",
    "        self.Y = Y\n",
    "        self.aZ = Y_pred\n",
    "        self.epsilon = 1e-40\n",
    "\n",
    "    def __str__(self, ):\n",
    "        return \"An instance of Cross Entropy Loss Layer\"\n",
    "\n",
    "    def forward(self, ):\n",
    "        self.L = - np.sum(self.Y * np.log(self.aZ+self.epsilon))\n",
    "\n",
    "    def backward(self, ):\n",
    "        self.dL_daZ = -1*(self.Y/(self.aZ + self.epsilon)\n",
    "                          ).T  # Element wise division\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActivation : \n",
    "    \"\"\"\n",
    "    Implementation of linear activation function.\n",
    "    Input : Z in R^(1xn)\n",
    "    Ouput : linear(Z) = Z \n",
    "    \"\"\"\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z \n",
    "        \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Linear Activation.\"\n",
    "    \n",
    "    def forward(self, ):\n",
    "        self.aZ = self.Z \n",
    "    \n",
    "    def backward(self,):\n",
    "        self.daZ_dZ = np.identity( self.Z.shape[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tanhActivation:\n",
    "    \"\"\"\n",
    "    Implementation of tanh activation function\n",
    "    Input : a numpy array Z in R^(1xK)\n",
    "    1. Forward Pass : aZ = tanh(Z)\n",
    "    2. Backward Pass : daZ/dZ = np.diag(1 - aZ**2)   --> R^(KxK)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z\n",
    "\n",
    "    def __str__(self,):\n",
    "        return \"An instance of tanhActivation class.\"\n",
    "\n",
    "    def forward(self,):\n",
    "        self.aZ = np.tanh(self.Z)\n",
    "\n",
    "    def backward(self,):\n",
    "        self.daZ_dZ = np.diag(1 - self.aZ.reshape(-1)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUActivation : \n",
    "    \"\"\"\n",
    "    Implementation of relu activatino function\n",
    "    Input : a numpy array Z in R^(1xK)\n",
    "    1. Forward Pass aZ = max(Z,0)\n",
    "    2. Backward Pass : daZ_dZ = diag_matrix( 1 if aZ_i>0 else 0 )\n",
    "    \"\"\"\n",
    "    def __init__(self, Z): \n",
    "        self.Z = Z \n",
    "        self.Leak = 0.01\n",
    "    \n",
    "    def __str__(self,):\n",
    "        return \"An instance of ReLU activation\"\n",
    "    \n",
    "    def forward(self,):\n",
    "        self.aZ = np.maximum(self.Z,0)\n",
    "    \n",
    "    def backward(self,):\n",
    "        self.daZ_dZ = np.diag( [1. if x>=0 else self.Leak for x in self.aZ.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name='boston',\n",
    "              normalize_X=False,\n",
    "              normalize_y=False,\n",
    "              one_hot_encode_y=False,\n",
    "              test_size=0.2):\n",
    "    if dataset_name == 'boston':\n",
    "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "        data_boston = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "        result_boston = raw_df.values[1::2, 2]\n",
    "        data = {'data': data_boston, 'target': result_boston}\n",
    "        \n",
    "\n",
    "        # data = load_boston()\n",
    "    elif dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'mnist':\n",
    "        data = load_digits()\n",
    "        data['data'] = 1*(data['data'] >= 8)\n",
    "\n",
    "    X = data['data']\n",
    "    y = data['target'].reshape(-1, 1)\n",
    "\n",
    "    if normalize_X == True:\n",
    "        normalizer = Normalizer()\n",
    "        X = normalizer.fit_transform(X)\n",
    "\n",
    "    if normalize_y == True:\n",
    "        normalizer = Normalizer()\n",
    "        y = normalizer.fit_transform(y)\n",
    "\n",
    "    if one_hot_encode_y == True:\n",
    "        encoder = OneHotEncoder()\n",
    "        y = encoder.fit_transform(y).toarray()\n",
    "        # y = np.eye(3)[y.reshape(-1)]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size)\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Input - activation : Activation Layer Name ,n_inp : dimension of input ,  n_out :  Number of output neurons \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inp, n_out, activation_name=\"linear\", seed=42):\n",
    "\n",
    "        np.random.seed(seed)  # for reproducability of code\n",
    "\n",
    "        self.n_inp = n_inp\n",
    "        self.n_out = n_out\n",
    "\n",
    "        # random initialization of input X  and output Z\n",
    "        self.X = np.random.random((1, n_inp))   # assigned during SGD\n",
    "        self.Z = np.random.random((1, n_out))\n",
    "\n",
    "        # Initialize W & B with some scaling to avoid over-flow\n",
    "        self.W = np.random.random((n_inp, n_out)) * \\\n",
    "            np.sqrt(2 / (n_inp + n_out))\n",
    "        self.B = np.random.random((1, n_out))*np.sqrt(2 / (1 + n_out))\n",
    "\n",
    "        # define multiplication layer, bias addition layer , and activation layer\n",
    "        self.multiply_layer = MultiplicationLayer(self.X, self.W)\n",
    "        self.bias_add_layer = BiasAdditionLayer(self.B, self.B)\n",
    "\n",
    "        if activation_name == 'linear':\n",
    "            self.activation_layer = LinearActivation(self.Z)\n",
    "        elif activation_name == 'sigmoid':\n",
    "            self.activation_layer = SigmoidActivation(self.Z)\n",
    "        elif activation_name == 'softmax':\n",
    "            self.activation_layer = SoftMaxActivation(self.Z)\n",
    "        elif activation_name == 'tanh':\n",
    "            self.activation_layer = tanhActivation(self.Z)\n",
    "        elif activation_name == 'relu':\n",
    "            self.activation_layer = ReLUActivation(self.Z)\n",
    "\n",
    "    def forward(self,):\n",
    "        self.multiply_layer.X = self.X\n",
    "        self.multiply_layer.forward()\n",
    "\n",
    "        self.bias_add_layer.Z = self.multiply_layer.Z\n",
    "        self.bias_add_layer.forward()\n",
    "\n",
    "        self.activation_layer.Z = self.bias_add_layer.Z\n",
    "        self.activation_layer.forward()\n",
    "\n",
    "        self.Z = self.activation_layer.aZ  # output of given layer\n",
    "\n",
    "    def backward(self,):\n",
    "        self.activation_layer.backward()\n",
    "        self.bias_add_layer.backward()\n",
    "        self.multiply_layer.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(Layer):\n",
    "    \"\"\"\n",
    "    Input  - layers : list of layer objects , loss_name : Name of loss layer\n",
    "    \"\"\"\n",
    "\n",
    "    # [ \"mean_squared\", \"cross_entropy\"]\n",
    "    def __init__(self, layers, loss_name=\"mean_squared\", learning_rate=0.01, seed=42):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)  # number of layers in neural network\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.inp_shape = self.layers[0].X.shape\n",
    "        self.out_shape = self.layers[-1].Z.shape\n",
    "\n",
    "        # random initialization of input X  and output Z\n",
    "        self.X = np.random.random(self.inp_shape)   # assigned during SGD\n",
    "        self.Y = np.random.random(self.out_shape)  # output of neural network\n",
    "\n",
    "        # define loss layer\n",
    "        if loss_name == \"mean_squared\":\n",
    "            self.loss_layer = MeanSquaredLossLayer(self.Y, self.Y)\n",
    "        if loss_name == \"cross_entropy\":\n",
    "            self.loss_layer = CrossEntropyLossLayer(self.Y, self.Y)\n",
    "\n",
    "    def forward(self,):\n",
    "        self.layers[0].X = self.X\n",
    "        self.loss_layer.Y = self.Y\n",
    "\n",
    "        self.layers[0].forward()\n",
    "        for i in range(1, self.n_layers):\n",
    "            self.layers[i].X = self.layers[i-1].Z\n",
    "            self.layers[i].forward()\n",
    "\n",
    "        self.loss_layer.aZ = self.layers[-1].Z\n",
    "        self.loss_layer.forward()\n",
    "\n",
    "    def backward(self,):\n",
    "\n",
    "        self.loss_layer.Z = self.Y\n",
    "        self.loss_layer.backward()\n",
    "        self.grad_nn = self.loss_layer.dL_daZ\n",
    "        for i in range(self.n_layers-1, -1, -1):\n",
    "            self.layers[i].backward()\n",
    "\n",
    "            dL_dZ = np.dot(\n",
    "                self.layers[i].activation_layer.daZ_dZ, self.grad_nn)\n",
    "            dL_dW = np.dot(self.layers[i].multiply_layer.dZ_dW, dL_dZ.T)\n",
    "            dL_dB = np.dot(self.layers[i].bias_add_layer.dZ_dB, dL_dZ).T\n",
    "\n",
    "            # Update W & B\n",
    "            self.layers[i].W -= self.learning_rate*dL_dW\n",
    "            self.layers[i].B -= self.learning_rate*dL_dB\n",
    "\n",
    "            # Update outer_grad\n",
    "            self.grad_nn = np.dot(\n",
    "                self.layers[i].multiply_layer.dZ_daZ_prev, dL_dZ)\n",
    "\n",
    "            del dL_dZ, dL_dW, dL_dB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLayers(inp_shape, layers_sizes, layers_activations):\n",
    "    layers = []\n",
    "    n_layers = len(layers_sizes)\n",
    "    layer_0 = Layer(inp_shape, layers_sizes[0], layers_activations[0])\n",
    "    layers.append(layer_0)\n",
    "    inp_shape_next = layers_sizes[0]\n",
    "    for i in range(1, n_layers):\n",
    "        layer_i = Layer(inp_shape_next, layers_sizes[i], layers_activations[i])\n",
    "        layers.append(layer_i)\n",
    "        inp_shape_next = layers_sizes[i]\n",
    "\n",
    "    out_shape = inp_shape_next\n",
    "    return inp_shape, out_shape, layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_NeuralNetwork(X_train,\n",
    "                      y_train,\n",
    "                      X_test,\n",
    "                      y_test,\n",
    "                      nn,\n",
    "                      inp_shape=1,   # dimension of input\n",
    "                      out_shape=1,   # dimension of output\n",
    "                      n_iterations=1000,\n",
    "                      task=\"regression\"  # [ \"regression\", \"classification\"]\n",
    "                      ):\n",
    "    iterations = trange(n_iterations, desc=\"Training ...\", ncols=100)\n",
    "\n",
    "    for iteration, _ in enumerate(iterations):\n",
    "        randomIndx = np.random.randint(len(X_train))\n",
    "        X_sample = X_train[randomIndx, :].reshape(1, inp_shape)\n",
    "        Y_sample = y_train[randomIndx, :].reshape(1, out_shape)\n",
    "\n",
    "        nn.X = X_sample\n",
    "        nn.Y = Y_sample\n",
    "\n",
    "        nn.forward()  # Forward Pass\n",
    "        nn.backward()  # Backward Pass\n",
    "\n",
    "    # Lets run ONLY forward pass for train and test data and check accuracy/error\n",
    "\n",
    "    if task == \"regression\":\n",
    "        nn.X = X_train\n",
    "        nn.Y = y_train\n",
    "        nn.forward()\n",
    "        train_error = nn.loss_layer.L\n",
    "        nn.X = X_test\n",
    "        nn.Y = y_test\n",
    "\n",
    "        nn.forward()\n",
    "\n",
    "        test_error = nn.loss_layer.L\n",
    "\n",
    "        if isinstance(nn.loss_layer, MeanSquaredLossLayer):\n",
    "            print(\"Mean Squared Loss Error (Train Data)  : %0.5f\" % train_error)\n",
    "            print(\"Mean Squared Loss Error (Test Data)  : %0.5f\" % test_error)\n",
    "\n",
    "    if task == \"classification\":\n",
    "        nn.X = X_train\n",
    "        nn.Y = y_train\n",
    "        nn.forward()\n",
    "        y_true = np.argmax(y_train, axis=1)\n",
    "        y_pred = np.argmax(nn.loss_layer.aZ, axis=1)\n",
    "        acc = 1*(y_true == y_pred)\n",
    "        print(\"Classification Accuracy (Training Data ): {0}/{1} = {2} %\".format(\n",
    "            sum(acc), len(acc), sum(acc)*100/len(acc)))\n",
    "\n",
    "        nn.X = X_test\n",
    "        nn.Y = y_test\n",
    "        nn.forward()\n",
    "        y_true = np.argmax(y_test, axis=1)\n",
    "        y_pred = np.argmax(nn.loss_layer.aZ, axis=1)\n",
    "        acc = 1*(y_true == y_pred)\n",
    "        print(\"Classification Accuracy (Testing Data ): {0}/{1} = {2} %\".format(\n",
    "            sum(acc), len(acc), sum(acc)*100/len(acc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data('boston', normalize_X=True, normalize_y=False, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One output neural with Linear activation and least mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|█████████████████████████████████████████| 11111/11111 [00:01<00:00, 7642.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 48.00306\n",
      "Mean Squared Loss Error (Test Data)  : 75.11091\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [1]\n",
    "layers_activations = ['linear']\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mean_squared'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.1)\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=11111,task=\"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 layers. Layer 1 with 13 output neurons with sigmoid activation. Layer 2 with one output neuron and linear activation. use mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|███████████████████████████████████████████| 1000/1000 [00:00<00:00, 7959.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 58.02224\n",
      "Mean Squared Loss Error (Test Data)  : 86.06957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [13,1]\n",
    "layers_activations = ['sigmoid','linear']\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mean_squared'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.01)\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=1000,task=\"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three layers. Layer 1 with 13 output neurons with sigmoid activation. Layer 2 with 13 output neurons and sigmoid activation. Layer 3 with one output neuron and linear activation. use mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|███████████████████████████████████████████| 1000/1000 [00:00<00:00, 6571.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 79.62934\n",
      "Mean Squared Loss Error (Test Data)  : 107.37404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [13,13,1]\n",
    "layers_activations = ['sigmoid','sigmoid','linear']\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mean_squared'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.001)\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=1000,task=\"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data('mnist', one_hot_encode_y=True, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two layers. Layer 1 with 89 output neurons with tanh activation. Layer 2 with ten output neuron and sigmoid activation. use mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|█████████████████████████████████████████| 10000/10000 [00:01<00:00, 7283.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Training Data ): 1194/1257 = 94.98806682577566 %\n",
      "Classification Accuracy (Testing Data ): 490/540 = 90.74074074074075 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [89,10]\n",
    "layers_activations = ['tanh','sigmoid']\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mean_squared'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.1)\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=10000,task=\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two layers. Layer 1 with 89 output neurons with tanh activation. Layer 2 with ten output neuron and linear activation. use softmax with cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|█████████████████████████████████████████| 10000/10000 [00:01<00:00, 7491.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Training Data ): 1191/1257 = 94.74940334128878 %\n",
      "Classification Accuracy (Testing Data ): 497/540 = 92.03703703703704 %\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [89,10]\n",
    "layers_activations = ['tanh','softmax']\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'cross_entropy'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.01)\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=10000,task=\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we are given single channel input and initial filter to be a 3x3 matrix:\n",
    "\n",
    "def convolutional_layer(zero_pad_input, l_filter):\n",
    "\n",
    "    l = len(inp)  # length of input matrix\n",
    "    m = len(l_filter)  # length of filter\n",
    "    c = len(zero_pad_input)  # size of zero-padded matrix\n",
    "    s = (c - m) + 1  # to be used for loop for filtering\n",
    "    out = np.zeros((l, l))  # output after convolution\n",
    "\n",
    "    # filtering-\n",
    "    for i in range(s):\n",
    "        for j in range(s):\n",
    "            temp = np.zeros((m, m))\n",
    "            row, col = np.indices((m, m))\n",
    "            temp = np.multiply(zero_pad_input[row+i, col+j], l_filter)\n",
    "\n",
    "            out[i][j] = np.sum(temp)\n",
    "\n",
    "    return out\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Forward pass implementation-\n",
    "def Forward_pass(inp, l_filter):\n",
    "    l = len(inp)\n",
    "    # Zero-padding of input layer-\n",
    "    zero_pad_input = np.zeros((l+2, l+2))\n",
    "    zero_pad_input[1:l+1, 1:l+1] = inp\n",
    "\n",
    "    f_out = convolutional_layer(zero_pad_input, l_filter)\n",
    "    return f_out\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# Function to Rotate\n",
    "# the matrix by 180 degree\n",
    "def rotateMatrix(mat):\n",
    "    N = len(mat)\n",
    "    rot_mat = np.zeros((N, N))\n",
    "    k = N - 1\n",
    "    t1 = 0\n",
    "    while (k >= 0 and t1 < 3):\n",
    "        j = N - 1\n",
    "        t2 = 0\n",
    "        while (j >= 0 and t2 < N):\n",
    "            rot_mat[t1][t2] = mat[k][j]\n",
    "            j = j - 1\n",
    "            t2 = t2 + 1\n",
    "        k = k - 1\n",
    "        t1 = t1 + 1\n",
    "\n",
    "    return rot_mat\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Backward pass implementation-\n",
    "\n",
    "def Backward_pass(inp, output, l_filter):\n",
    "    l = len(inp)\n",
    "\n",
    "# --------------------------------Backward Pass---------------------------------------\n",
    "    # Zero-padding of input layer-\n",
    "    zero_pad_input = np.zeros((l+2, l+2))\n",
    "    zero_pad_input[1:l+1, 1:l+1] = inp\n",
    "\n",
    "    grad_filter = convolutional_layer(zero_pad_input, output)\n",
    "    # we can use gradient of filter coefficient matrix to update the filter matrix:\n",
    "    # -- l_filter - l_filter - alpha*grad_filter ,where alpha is learning rate\n",
    "\n",
    "    # for gradient of loss w.r.t input, we need to rotate the filter by 180° and apply convolution.\n",
    "    rotated_filter = rotateMatrix(l_filter)\n",
    "    zero_pad_output = np.zeros((l+2, l+2))\n",
    "    zero_pad_output[1:l+1, 1:l+1] = output\n",
    "    grad_X = convolutional_layer(zero_pad_output, rotated_filter)\n",
    "\n",
    "    return grad_filter, grad_X\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# flatten operation:\n",
    "\n",
    "def flatten(inp_mat):\n",
    "    flatten_vector = []\n",
    "\n",
    "    for i in range(len(inp_mat)):  # number of rows\n",
    "        for j in range(len(inp_mat[0])):  # number of columns\n",
    "            flatten_vector.append(inp_mat[i][j])\n",
    "\n",
    "    flatten_vector = np.array(flatten_vector)\n",
    "    return flatten_vector\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer:\n",
    "    \"\"\"\n",
    "    Implementation of Convolutional Layer consist of Convolution  followed by flattening  and Activation operation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # inp_shape = (input_channels, input_height, input_width )\n",
    "                 inp_shape,\n",
    "                 activation='tanh',\n",
    "                 # filter_shape = (filter_height, filter_width)\n",
    "                 filter_shape=(1, 1),\n",
    "                 lr=0.01,\n",
    "                 Co=1,\n",
    "                 seed=42):                                                # number of output channels\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        # Check if filter is valid or NOT\n",
    "        assert (inp_shape[1] >= filter_shape[0] and inp_shape[2] >= filter_shape[1]), \\\n",
    "            \"Error : Input {} incompatible with filter {}\".format(\n",
    "                inp.shape, filter_shape)\n",
    "\n",
    "        self.inp = np.random.rand(*inp_shape)\n",
    "        self.inp_shape = inp_shape\n",
    "        # number of channels in input here denoted as inp\n",
    "        self.Ci = self.inp.shape[0]\n",
    "        # number of output channels\n",
    "        self.Co = Co\n",
    "        self.filters_shape = (self.Co, self.Ci,  *filter_shape)\n",
    "        self.out_shape = (\n",
    "            self.Co, self.inp.shape[1] - filter_shape[0] + 1, self.inp.shape[2] - filter_shape[1] + 1)\n",
    "        self.flatten_shape = self.out_shape[0] * \\\n",
    "            self.out_shape[1]*self.out_shape[2]\n",
    "        self.lr = lr\n",
    "\n",
    "        # Randomly initialize filters, biases, output, flatten output\n",
    "        self.filters = np.random.rand(*self.filters_shape)\n",
    "        self.biases = np.random.rand(*self.out_shape)\n",
    "        self.out = np.random.rand(*self.out_shape)\n",
    "        self.flatten_out = np.random.rand(1, self.flatten_shape)\n",
    "\n",
    "        # Define activation function\n",
    "        if activation == 'tanh':\n",
    "            self.activation_layer = tanhActivation(self.out)\n",
    "\n",
    "    def forward(self, ):\n",
    "        self.out = np.copy(self.biases)  # add bias to output\n",
    "        for i in range(self.Co):\n",
    "            for j in range(self.Ci):\n",
    "                self.out[i] += self.convolve(self.inp[j], self.filters[i, j])\n",
    "\n",
    "        self.flatten()\n",
    "        self.activation_layer.Z = self.flatten_out\n",
    "        self.activation_layer.forward()\n",
    "\n",
    "    def backward(self, grad_nn):\n",
    "\n",
    "        self.activation_layer.backward()\n",
    "        loss_gradient = np.dot(self.activation_layer.daZ_dZ, grad_nn)\n",
    "        # reshape to (Co, H_out, W_out)\n",
    "        loss_gradient = np.reshape(loss_gradient, self.out_shape)\n",
    "\n",
    "        # dL/dKij for each filter  Kij    1<=i<=Ci , 1<=j<=Co\n",
    "        self.filters_gradient = np.zeros(self.filters_shape)\n",
    "        self.input_gradient = np.zeros(self.inp_shape)  # dL/dXj\n",
    "        self.biases_gradient = loss_gradient  # dL/dBi  = dL/dYi\n",
    "        padded_loss_gradient = np.pad(loss_gradient, ((\n",
    "            0, 0), (self.filters_shape[2]-1, self.filters_shape[2]-1), (self.filters_shape[3]-1, self.filters_shape[3]-1)))\n",
    "\n",
    "        for i in range(self.Co):\n",
    "            for j in range(self.Ci):\n",
    "                self.filters_gradient[i, j] = self.convolve(\n",
    "                    self.inp[j], loss_gradient[i])  # dL/dKij = convolution( Xj, dL/dYi)\n",
    "                rot180_Kij = np.rot90(\n",
    "                    np.rot90(self.filters[i, j], axes=(0, 1)), axes=(0, 1))\n",
    "                # dL/dXj = convolution ( padded dL/dYi , Kij rotated by 180 anit-clockwise )\n",
    "                self.input_gradient[j] += self.convolve(\n",
    "                    padded_loss_gradient[i], rot180_Kij)\n",
    "\n",
    "        # update filters and biases\n",
    "        self.filters -= self.lr*self.filters_gradient\n",
    "        self.biases -= self.lr*self.biases_gradient\n",
    "\n",
    "    # flattening output to 1 Dimension so it can be fed int neural network\n",
    "\n",
    "    def flatten(self, ):\n",
    "        self.flatten_out = self.out.reshape(1, -1)\n",
    "\n",
    "    # convolutional operation with stride=1\n",
    "    def convolve(self, x, y):\n",
    "        x_conv_y = np.zeros(\n",
    "            (x.shape[0] - y.shape[0] + 1, x.shape[1] - y.shape[1] + 1))\n",
    "        for i in range(x.shape[0]-y.shape[0] + 1):\n",
    "            for j in range(x.shape[1] - y.shape[1] + 1):\n",
    "                tmp = x[i:i+y.shape[0], j:j+y.shape[1]]\n",
    "                tmp = np.multiply(tmp, y)\n",
    "                x_conv_y[i, j] = np.sum(tmp)\n",
    "        return x_conv_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN : \n",
    "    \"\"\"\n",
    "    Implementation of Convolutional Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                convolutional_layer,                   # convolutional layer \n",
    "                nn,                                    # feed forward neural network\n",
    "                seed = 42): \n",
    "\n",
    "        self.nn = nn \n",
    "        self.convolutional_layer = convolutional_layer \n",
    "        self.X = _ # assigned during SGD \n",
    "        self.Y = _ # assigned during SGD \n",
    "    \n",
    "    def forward(self,):\n",
    "        # forward pass of convolutional layer \n",
    "        self.convolutional_layer.inp = self.X \n",
    "        self.convolutional_layer.forward()\n",
    "\n",
    "        # forward pass of neural network \n",
    "        self.nn.X = self.convolutional_layer.activation_layer.aZ\n",
    "        self.nn.Y = self.Y \n",
    "        self.nn.forward()  \n",
    "    \n",
    "    def backward(self,): \n",
    "        # backward pass of neural network \n",
    "        self.nn.backward() \n",
    "\n",
    "        # backward pass of convolutional network \n",
    "        self.convolutional_layer.backward( self.nn.grad_nn )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_CNN(X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            cnn,\n",
    "            inp_shape,\n",
    "            out_shape,\n",
    "            n_iterations=1000,\n",
    "            task=\"classification\"):\n",
    "\n",
    "    iterations = trange(n_iterations, desc=\"Training ...\", ncols=100)\n",
    "\n",
    "    for iteration, _ in enumerate(iterations):\n",
    "        randomIndx = np.random.randint(len(X_train))\n",
    "        X_sample = X_train[randomIndx, :].reshape(inp_shape)\n",
    "        Y_sample = y_train[randomIndx, :].reshape(out_shape)\n",
    "\n",
    "        cnn.X = X_sample\n",
    "        cnn.Y = Y_sample\n",
    "\n",
    "        cnn.forward()  # Forward Pass\n",
    "        cnn.backward()  # Backward Pass\n",
    "\n",
    "    # Lets run ONLY forward pass for train and test data and check accuracy/error\n",
    "\n",
    "    if task == \"classification\":\n",
    "        X_train = X_train.reshape(-1, 8, 8)\n",
    "        y_true = np.argmax(y_train, axis=1)\n",
    "        acc = 0\n",
    "        for i in range(len(X_train)):\n",
    "            cnn.X = X_train[i][np.newaxis, :, :]\n",
    "            cnn.Y = y_train[i]\n",
    "            cnn.forward()\n",
    "            y_pred_i = np.argmax(cnn.nn.loss_layer.aZ, axis=1)\n",
    "            if (y_pred_i == y_true[i]):\n",
    "                acc += 1\n",
    "        \n",
    "        print(\"Classification Accuracy (Training Data ):\" + str(acc) + \"/\" + str(len(y_true)) + \" = \" + str(acc*100/len(y_true)) + \" %\" )\n",
    "\n",
    "        X_test = X_test.reshape(-1, 8, 8)\n",
    "        y_true = np.argmax(y_test, axis=1)\n",
    "        acc = 0\n",
    "        for i in range(len(X_test)):\n",
    "            cnn.X = X_test[i][np.newaxis, :, :]\n",
    "            cnn.Y = y_test[i]\n",
    "            cnn.forward()\n",
    "            y_pred_i = np.argmax(cnn.nn.loss_layer.aZ, axis=1)\n",
    "            if (y_pred_i == y_true[i]):\n",
    "                acc += 1\n",
    "        \n",
    "        print(\"Classification Accuracy (Testing Data ):\" + str(acc) + \"/\" + str(len(y_true)) + \" = \" + str(acc*100/len(y_true)) + \" %\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data('mnist', one_hot_encode_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|█████████████████████████████████████████████| 5000/5000 [03:53<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Training Data ): 1242/1437 = 86.43006263048017 %\n",
      "Classification Accuracy (Testing Data ): 304/360 = 84.44444444444444 %\n"
     ]
    }
   ],
   "source": [
    "conv_inp_shape = (1,8,8)   # sklearn digit dataset has images of shape 1 x 8 x 8\n",
    "Co = 16  # 16 channel output \n",
    "conv_filter_shape = (3,3)\n",
    "conv_activation = 'tanh'\n",
    "convolutional_layer = ConvolutionalLayer(conv_inp_shape, \n",
    "                                        filter_shape = conv_filter_shape, \n",
    "                                        Co = Co,\n",
    "                                        activation = conv_activation,\n",
    "                                        lr = 0.01)\n",
    "nn_inp_shape = convolutional_layer.flatten_shape \n",
    "layers_sizes = [10]\n",
    "layers_activations = ['softmax']\n",
    "\n",
    "nn_inp_shape, nn_out_shape, layers = createLayers(nn_inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'cross_entropy'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.01)\n",
    "\n",
    "cnn = CNN( convolutional_layer, nn)\n",
    "out_shape =  (1, layers_sizes[-1])  # one_hot encoded ouptut \n",
    "\n",
    "SGD_CNN(X_train,y_train,X_test,y_test, cnn,conv_inp_shape, out_shape,n_iterations=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
