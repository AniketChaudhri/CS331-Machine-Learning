{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do install Follwoing libraries (if not already installed ) for smooth working of code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_boston , load_iris, load_digits\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m Normalizer, OneHotEncoder\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n",
      "File \u001b[0;32m~/anaconda3/envs/cs331/lib/python3.10/site-packages/sklearn/datasets/__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mload_boston\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     msg \u001b[39m=\u001b[39m textwrap\u001b[39m.\u001b[39mdedent(\n\u001b[1;32m    107\u001b[0m         \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m        \"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[1;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mglobals\u001b[39m()[name]\n",
      "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston , load_iris, load_digits\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import trange "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "\n",
    "1. Matrix Multiplication Layer\n",
    "2. Bias Addition Layer \n",
    "3. Mean Squared loss layer \n",
    "4. Softmax Activation \n",
    "5. Sigmoid Activation \n",
    "5. Cross Entropy Loss Layer \n",
    "6. Linear Activation \n",
    "7. tanh Activation \n",
    "8. ReLU Activation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a837fb",
   "metadata": {},
   "source": [
    "**1. Matrix Multiplication Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5416ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicationLayer : \n",
    "    \"\"\"\n",
    "    Inputs : X in R^(1xd) , W in R^(dxK)\n",
    "    This layer takes X & W as input and perform these 2 tasks: \n",
    "    1. Forward Pass : Matrix multiplication,  Z = XW \n",
    "    2. Backward Pass : dZ/dX , dZ/dW \n",
    "    \"\"\"\n",
    "    def __init__(self, X, W) : \n",
    "        self.X = X \n",
    "        self.W = W \n",
    "\n",
    "    def __str__(self,):\n",
    "        return \" An instance of Muliplication Layer.\"\n",
    "\n",
    "    def forward(self):  \n",
    "        self.Z = np.dot(self.X, self.W)\n",
    "\n",
    "    def backward(self):\n",
    "        self.dZ_dW = (self.X).T  # dZ/dW \n",
    "        self.dZ_daZ_prev = self.W  # dZ/dX "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546dc812",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f4e110",
   "metadata": {},
   "source": [
    "**2 Bias Addition Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c881f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdditionLayer : \n",
    "    \"\"\"\n",
    "    Inputs : Z in R^(1xK), B in R^(1xK)\n",
    "    This layer takes output Z of forward pass of Multiplication Layer as input and perform these 2 operations : \n",
    "    1. Forward Pass :  Z = Z + B\n",
    "    2. Backward Pass : dZ/dB\n",
    "    \"\"\"\n",
    "    def __init__(self, Z : np.ndarray , bias : np.ndarray ):\n",
    "        self.B = bias\n",
    "        self.Z = Z\n",
    "    \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Bias Addition Layer.\"\n",
    "    \n",
    "    def forward(self,):\n",
    "        self.Z = self.Z + self.B\n",
    "    \n",
    "    def backward(self,):\n",
    "        self.dZ_dB = np.identity( self.B.shape[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e58058",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec113e",
   "metadata": {},
   "source": [
    "**3. Mean Squared Loss Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f476834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredLossLayer : \n",
    "    \"\"\"\n",
    "    This layer implements Mean Square Loss Layer.\n",
    "    Inputs : Y in R^(1xK) , Y_hat in R^(1xK)  where K --> dimesion of output layer \n",
    "    This layer takes prediction Y_hat and true Y as input and perform these 2 opearations : \n",
    "    1. Forward Pass : L = (1/n) * || Y_hat - Y||**2 \n",
    "    2. Backward Pass : dL/dY_hat = (2/n)*(Y_hat - Y).T   Note :Here instead of dL/dY_hat , I used dL/daZ symbol which denote \n",
    "                                                             derivative of loss w.r.t. output of previous activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self, Y : np.ndarray , Y_hat : np.ndarray):\n",
    "        self.Y = Y \n",
    "        self.aZ = Y_hat \n",
    "    \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Mean Squared Loss Layer\"\n",
    "    \n",
    "    def forward(self, ):\n",
    "        self.L = np.mean( ( self.aZ - self.Y)**2 )\n",
    "        \n",
    "    def backward(self,):\n",
    "        self.dL_daZ = (2/len(self.Y))*(self.aZ - self.Y).T      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a381e7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca37b5d4",
   "metadata": {},
   "source": [
    "**4. Soft Max Activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed79a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxActivation : \n",
    "    \"\"\"\n",
    "    This layer implements SoftMax Activation Function.\n",
    "    Input : a numpy array Z in R^(1XK)  \n",
    "    1. Forward Pass : Apply Softmax Activation function, aZ = softmax(Z).T\n",
    "    2. Backward Pass : daZ/dZ  = diag(aZ) - sZ*transpose(aZ)  --> here diag(aZ) is diagonal matrix with \n",
    "                                                                   i-th diagnoal entry replaced by sZ_i value\n",
    "    \"\"\"\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z \n",
    "        \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Softmax Activation Layer\"\n",
    "        \n",
    "    def forward(self,):\n",
    "        self.aZ = self.softmax(self.Z)\n",
    "    \n",
    "    def backward(self,):\n",
    "        self.daZ_dZ = np.diag( self.aZ.reshape(-1) ) - (self.aZ.T)@( (self.aZ))  # Shape = (K,K) where K = len( sZ )\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(Z : np.ndarray):\n",
    "        max_Z = np.max( Z, axis=1 ,keepdims=True )\n",
    "        return (np.exp(Z - max_Z ))/np.sum( np.exp(Z - max_Z), axis=1 , keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b04aa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa13ef",
   "metadata": {},
   "source": [
    "**5. Sigmoid Activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60de0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidActivation :\n",
    "    \"\"\"\n",
    "    This layer implements Sigmoid Activation Function. \n",
    "    Input : a numpy array Z of shape Kx1 \n",
    "    1. Forward Pass : aZ = sigmoid( Z )  \n",
    "    2. Backward Pass : daZ/dZ = diagonal matrix with entries aZ_i*(1-aZ_i) --> sigZ_i means i-th component of sigZ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,Z ):\n",
    "        self.Z = Z \n",
    "    \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Sigmoid Activation Layer\"\n",
    "    \n",
    "    def forward(self,):\n",
    "        self.aZ = self.sigmoid( self.Z )  # sigmoid calculation\n",
    "    \n",
    "    def backward(self,):\n",
    "        diag_entries = np.multiply(self.aZ, 1-self.aZ).reshape(-1)\n",
    "        self.daZ_dZ = np.diag(diag_entries) \n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid( Z : np.ndarray ) :\n",
    "        return  1./(1 + np.exp(-Z) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa07a4c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement combination of sigmoid and softmax activation function\n",
    "class SigmoidSoftmaxActivation :\n",
    "    \"\"\"\n",
    "    This layer implements combination of Sigmoid and Softmax Activation Function. \n",
    "    Input : a numpy array Z of shape Kx1 \n",
    "    1. Forward Pass : aZ = sigmoid( Z )  \n",
    "    2. Backward Pass : daZ/dZ = diagonal matrix with entries aZ_i*(1-aZ_i) --> sigZ_i means i-th component of sigZ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,Z ):\n",
    "        self.Z = Z \n",
    "    \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Sigmoid Activation Layer\"\n",
    "    \n",
    "    def forward(self,):\n",
    "        self.aZ = self.sigmoid( self.Z )  # sigmoid calculation\n",
    "    \n",
    "    def backward(self,):\n",
    "        diag_entries = np.multiply(self.aZ, 1-self.aZ).reshape(-1)\n",
    "        self.daZ_dZ = np.diag(diag_entries) \n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid( Z : np.ndarray ) :\n",
    "        return  1./(1 + np.exp(-Z) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb583e9",
   "metadata": {},
   "source": [
    "**6. Cross Entropy Loss Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd7235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossLayer : \n",
    "    \"\"\"\n",
    "    This layer implements Cross Entropy Loss Layer. \n",
    "    Inputs : Y in R^(1xK) , Y_pred in R^(1xK)  where K --> dimesion of output layer \n",
    "    This layer takes prediction Y_pred and true Y as input and perform these 2 opearations : \n",
    "    1. Forward Pass : L = -1 * dot product of Y & log(Y_pred)    \n",
    "    2. Backward Pass : dL/dY_pred in R^(Kx1)\n",
    "    \"\"\"    \n",
    "    def __init__(self, Y , Y_pred): \n",
    "        self.Y = Y\n",
    "        self.aZ = Y_pred\n",
    "        self.epsilon = 1e-40  \n",
    "        \n",
    "    \n",
    "    def __str__(self, ):\n",
    "        return \"An instance of Cross Entropy Loss Layer\"\n",
    "    \n",
    "    def forward(self, ):\n",
    "        self.L = - np.sum( self.Y * np.log(self.aZ+self.epsilon) )\n",
    "        \n",
    "    def backward(self, ):\n",
    "        self.dL_daZ = -1*(self.Y/(self.aZ + self.epsilon)).T # Element wise division"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824e020",
   "metadata": {},
   "source": [
    "**7. Linear Activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d14048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActivation : \n",
    "    \"\"\"\n",
    "    Implementation of linear activation function.\n",
    "    Input : Z in R^(1xn)\n",
    "    Ouput : linear(Z) = Z \n",
    "    \"\"\"\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z \n",
    "        \n",
    "    def __str__(self,):\n",
    "        return \"An instance of Linear Activation.\"\n",
    "    \n",
    "    def forward(self, ):\n",
    "        self.aZ = self.Z \n",
    "    \n",
    "    def backward(self,):\n",
    "        self.daZ_dZ = np.identity( self.Z.shape[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 & 3 \n",
    "\n",
    "- Question 2 :  Boston House Price Prediction \n",
    "- Question 3 :  IRIS Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data and Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name='boston', \n",
    "             normalize_X=False, \n",
    "             normalize_y=False,\n",
    "             one_hot_encode_y = False, \n",
    "             test_size=0.2):\n",
    "    if dataset_name == 'boston' : \n",
    "        data = load_boston()\n",
    "    elif dataset_name == 'iris' : \n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'mnist':\n",
    "        data = load_digits()\n",
    "        data['data'] = 1*(data['data']>=8)\n",
    "\n",
    "    X = data['data']\n",
    "    y = data['target'].reshape(-1,1)\n",
    "    \n",
    "    if normalize_X == True : \n",
    "        normalizer = Normalizer()\n",
    "        X  = normalizer.fit_transform(X)\n",
    "    \n",
    "    if normalize_y == True : \n",
    "        normalizer = Normalizer()\n",
    "        y = normalizer.fit_transform(y)\n",
    "    \n",
    "    if one_hot_encode_y == True : \n",
    "        # encoder = OneHotEncoder()\n",
    "        # y = encoder.fit_transform(y).toarray()\n",
    "        y = np.eye(3)[y.reshape(-1)]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent ( SGD )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_pass(X_sample, Y_sample, W, B, activation='linear', loss='mean_squared'):\n",
    "    multiply_layer = MultiplicationLayer(X_sample, W)\n",
    "    multiply_layer.forward()\n",
    "\n",
    "    bias_add_layer = BiasAdditionLayer(multiply_layer.Z, B)\n",
    "    bias_add_layer.forward()\n",
    "\n",
    "    if activation == 'linear' : \n",
    "        activation_layer = LinearActivation(bias_add_layer.Z)\n",
    "    elif activation == 'softmax': \n",
    "        activation_layer = SoftMaxActivation(bias_add_layer.Z)\n",
    "    activation_layer.forward()\n",
    "    \n",
    "    if loss == 'mean_squared' :\n",
    "        loss_layer = MeanSquaredLossLayer(Y_sample, activation_layer.aZ )\n",
    "    elif loss=='cross_entropy' : \n",
    "        loss_layer = CrossEntropyLossLayer(Y_sample, activation_layer.aZ )\n",
    "    loss_layer.forward()\n",
    "    \n",
    "    \n",
    "    return multiply_layer, bias_add_layer, activation_layer, loss_layer\n",
    "\n",
    "def backward_pass(multiply_layer, bias_add_layer, activation_layer, loss_layer): \n",
    "\n",
    "    loss_layer.backward()\n",
    "    activation_layer.backward()\n",
    "    bias_add_layer.backward()\n",
    "    multiply_layer.backward()\n",
    "\n",
    "    return loss_layer, activation_layer, bias_add_layer, multiply_layer \n",
    "\n",
    "\n",
    "def StochasticGradientDescent( X_train,\n",
    "                               y_train, \n",
    "                               X_test, \n",
    "                               y_test, \n",
    "                               inp_shape = 1,   # dimension of input \n",
    "                               out_shape = 1,   # dimension of output  \n",
    "                               n_iterations = 10000,\n",
    "                               learning_rate = 0.01,\n",
    "                               activation = 'linear',\n",
    "                               loss = 'mean_squared',\n",
    "                               seed = 42,\n",
    "                               task='regression'  #  one of  [ 'regression', 'classification' ]\n",
    "                            ):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # initialize W & B \n",
    "    W_shape = ( inp_shape,  out_shape )\n",
    "    B_shape = ( 1, out_shape )\n",
    "\n",
    "    W = np.random.random(W_shape)\n",
    "    B  = np.random.random(B_shape)\n",
    "\n",
    "    iterations = trange(n_iterations ,desc=\"Training...\", ncols=100)\n",
    "\n",
    "    for iteration, _ in enumerate(iterations) : \n",
    "        randomIndx = np.random.randint( len(X_train) )\n",
    "        X_sample = X_train[randomIndx, :].reshape(1, inp_shape)\n",
    "        Y_sample = y_train[randomIndx, :].reshape(1, out_shape)\n",
    "\n",
    "        # Forward Pass\n",
    "        # 1) Z <-- XW \n",
    "        # 2) Z <-- Z + Bias\n",
    "        # 3) Z <-- activation( Z ) \n",
    "        # 4) find Loss L \n",
    "\n",
    "        multiply_layer, bias_add_layer, activation_layer, loss_layer = forward_pass(X_sample, Y_sample, W, B, activation,loss)\n",
    "\n",
    "        # Note : here whenever I write aZ it means it is output of some activation function applied on Z \n",
    "\n",
    "        # Backward Pass \n",
    "        # 1) dL/daZ \n",
    "        # 2) dL/dZ = dL/daZ* daZ/dZ \n",
    "        # 3) dL/dW = dZ/dW * dL/dZ \n",
    "        # 4) dL/dB = dZ/dB * dL/dB \n",
    "        \n",
    "        loss_layer, activation_layer, bias_add_layer, multiply_layer = backward_pass(multiply_layer, bias_add_layer, activation_layer, loss_layer)\n",
    "        \n",
    "        dL_daZ = loss_layer.dL_daZ \n",
    "        dL_dZ = np.dot( activation_layer.daZ_dZ, dL_daZ ) \n",
    "        dL_dW = np.dot( multiply_layer.dZ_dW , dL_dZ.T)\n",
    "        dL_dB = np.dot( bias_add_layer.dZ_dB, dL_dZ).T\n",
    "\n",
    "        # Update W & B \n",
    "        W -=  learning_rate*dL_dW \n",
    "        B -=  learning_rate*dL_dB\n",
    "        \n",
    "        if iteration%1000 == 0 : \n",
    "            iterations.set_description( \"Sample Error : %0.5f\"%loss_layer.L, refresh=True )\n",
    "    \n",
    "    # Lets run forward pass for train and test data and check accuracy/error\n",
    "\n",
    "\n",
    "    if task =='regression':\n",
    "        if isinstance(loss_layer, MeanSquaredLossLayer) : \n",
    "            _ , _, _,  loss_layer = forward_pass( X_train, y_train , W, B, activation, loss)\n",
    "            print(\"Mean Squared Loss Error (Train Data)  : %0.5f\"% loss_layer.L)\n",
    "                        \n",
    "            _ , _, _,  loss_layer = forward_pass( X_test, y_test , W, B, activation, loss)\n",
    "            print(\"Mean Squared Loss error (Test Data) : %0.5f\"%loss_layer.L)\n",
    "    \n",
    "    if task =='classification': \n",
    "        if isinstance(loss_layer, CrossEntropyLossLayer): \n",
    "            y_true = np.argmax(y_train, axis=1)\n",
    "            _, _, _, loss_layer = forward_pass( X_train, y_train , W, B, activation, loss)\n",
    "            y_pred = np.argmax( loss_layer.aZ, axis=1)\n",
    "\n",
    "            acc = 1*(y_pred == y_true)\n",
    "            print(\"Classification Accuracy (Training Data ): {0}/{1} = {2} %\".format(sum(acc), len(acc), sum(acc)*100/len(acc)))\n",
    "\n",
    "            y_true = np.argmax(y_test,axis=1)\n",
    "            _, _, _, loss_layer = forward_pass( X_test, y_test , W, B, activation, loss)\n",
    "            y_pred = np.argmax( loss_layer.aZ, axis=1)\n",
    "\n",
    "            acc = 1*(y_pred == y_true)\n",
    "            print(\"Classification Accuracy (Testing Data ): {0}/{1} = {2} %\".format(sum(acc), len(acc), sum(acc)*100/len(acc)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7e1a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test  = load_data('boston', normalize_X=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample Error : 3.51963: 100%|██████████████████████████████| 10000/10000 [00:00<00:00, 27320.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 61.26899\n",
      "Mean Squared Loss error (Test Data) : 58.32278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "StochasticGradientDescent(X_train, y_train, X_test, y_test, inp_shape=X_train.shape[1], out_shape=y_train.shape[1], task='regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data('iris',normalize_X=True, one_hot_encode_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample Error : 0.89642: 100%|████████████████████████████████| 5000/5000 [00:00<00:00, 19016.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Training Data ): 79/120 = 65.83333333333333 %\n",
      "Classification Accuracy (Testing Data ): 21/30 = 70.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "StochasticGradientDescent(X_train,y_train,X_test,y_test, inp_shape=X_train.shape[1], \\\n",
    "                          out_shape=y_train.shape[1], \n",
    "                          n_iterations=5000,\n",
    "                          learning_rate=0.001,\n",
    "                          activation='softmax',\n",
    "                          task='classification',\n",
    "                          loss='cross_entropy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('cs331')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f43d1047cbef59950c2f25d14b04d8a01f164d85880f1357fcf1ba55dd5a278d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
